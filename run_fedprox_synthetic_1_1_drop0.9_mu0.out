2022-08-19 12:37:46.053276: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
WARNING:tensorflow:From /home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_1_1
	     drop_percent : 0.9
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  warnings.warn('`tf.layers.dense` is deprecated and '
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.
  warnings.warn('`layer.apply` is deprecated and '
2022-08-19 12:37:48.275372: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-08-19 12:37:48.276391: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-08-19 12:37:48.656448: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2022-08-19 12:37:48.656494: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node03): /proc/driver/nvidia/version does not exist
2022-08-19 12:37:48.656790: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-19 12:37:48.656879: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-08-19 12:37:48.658706: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)
2022-08-19 12:37:48.660134: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
WARNING:tensorflow:From /home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
Incomplete shape.
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================
Incomplete shape.
Incomplete shape.

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/4.84k flops)
  dense/kernel/Initializer/random_uniform (600/1.20k flops)
    dense/kernel/Initializer/random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/AssignSub (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  dense/kernel/Regularizer/Square (600/600 flops)
  dense/kernel/Regularizer/Sum (599/599 flops)
  PGD/update_dense/bias/AssignSub (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  dense/kernel/Regularizer/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
WARNING:tensorflow:From /home/aig/NailIt/FedProx/flearn/models/synthetic/mclr.py:58: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer Variable.assign which has equivalent behavior in 2.X.
At round 0 accuracy: 0.035977859778597784
At round 0 training accuracy: 0.035104166666666665
At round 0 training loss: 4.849575695159535
gradient difference: 146.9289105305644
At round 1 accuracy: 0.08671586715867159
At round 1 training accuracy: 0.09166666666666666
At round 1 training loss: 5.319398412058751
gradient difference: 164.8588238743799
At round 2 accuracy: 0.0996309963099631
At round 2 training accuracy: 0.105625
At round 2 training loss: 5.62493018321693
gradient difference: 163.25771034108283
At round 3 accuracy: 0.11254612546125461
At round 3 training accuracy: 0.1175
At round 3 training loss: 5.120532456822693
gradient difference: 162.84962994848286
At round 4 accuracy: 0.48985239852398527
At round 4 training accuracy: 0.5185416666666667
At round 4 training loss: 1.6025588710171481
gradient difference: 95.38512507345557
At round 5 accuracy: 0.488929889298893
At round 5 training accuracy: 0.48270833333333335
At round 5 training loss: 2.0723816284599406
gradient difference: 113.23954351374519
At round 6 accuracy: 0.5101476014760148
At round 6 training accuracy: 0.5433333333333333
At round 6 training loss: 1.8019508388948937
gradient difference: 105.23820667795306
At round 7 accuracy: 0.5322878228782287
At round 7 training accuracy: 0.5221875
At round 7 training loss: 1.519568465159585
gradient difference: 104.08280077513136
At round 8 accuracy: 0.5618081180811808
At round 8 training accuracy: 0.5909375
At round 8 training loss: 1.4685023171206315
gradient difference: 103.71829868930367
At round 9 accuracy: 0.5387453874538746
At round 9 training accuracy: 0.5617708333333333
At round 9 training loss: 1.6709937888911615
gradient difference: 108.0300489162954
At round 10 accuracy: 0.5571955719557196
At round 10 training accuracy: 0.5711458333333334
At round 10 training loss: 1.5192429586686194
gradient difference: 106.06615314996155
At round 11 accuracy: 0.5913284132841329
At round 11 training accuracy: 0.6148958333333333
At round 11 training loss: 1.362258366541937
gradient difference: 98.7529807967944
At round 12 accuracy: 0.577490774907749
At round 12 training accuracy: 0.6151041666666667
At round 12 training loss: 1.5417773103527725
gradient difference: 104.04893243660308
At round 13 accuracy: 0.5018450184501845
At round 13 training accuracy: 0.5270833333333333
At round 13 training loss: 1.7223013310569029
gradient difference: 115.29560253676847
At round 14 accuracy: 0.544280442804428
At round 14 training accuracy: 0.5810416666666667
At round 14 training loss: 1.746134572022905
gradient difference: 111.14559436344095
At round 15 accuracy: 0.5645756457564576
At round 15 training accuracy: 0.6011458333333334
At round 15 training loss: 1.7918505621204774
gradient difference: 111.18091398815092
At round 16 accuracy: 0.44649446494464945
At round 16 training accuracy: 0.4708333333333333
At round 16 training loss: 2.0894841728576767
gradient difference: 128.96312344363332
At round 17 accuracy: 0.4326568265682657
At round 17 training accuracy: 0.45666666666666667
At round 17 training loss: 2.0826546848053114
gradient difference: 129.64447955446857
At round 18 accuracy: 0.4511070110701107
At round 18 training accuracy: 0.4746875
At round 18 training loss: 1.8807646263887485
gradient difference: 123.7277418174515
At round 19 accuracy: 0.4317343173431734
At round 19 training accuracy: 0.4527083333333333
At round 19 training loss: 1.97768172179504
gradient difference: 127.19724839998396
At round 20 accuracy: 0.4575645756457565
At round 20 training accuracy: 0.4764583333333333
At round 20 training loss: 1.7346013385150583
gradient difference: 119.86536193333545
At round 21 accuracy: 0.4981549815498155
At round 21 training accuracy: 0.5135416666666667
At round 21 training loss: 1.6027833326005687
gradient difference: 113.9950232533945
At round 22 accuracy: 0.45387453874538747
At round 22 training accuracy: 0.4713541666666667
At round 22 training loss: 1.9028117628628387
gradient difference: 122.0875265363343
At round 23 accuracy: 0.566420664206642
At round 23 training accuracy: 0.5852083333333333
At round 23 training loss: 1.4907953838010628
gradient difference: 110.43592481234613
At round 24 accuracy: 0.5931734317343174
At round 24 training accuracy: 0.610625
At round 24 training loss: 1.5072282085443536
gradient difference: 110.81165950558972
At round 25 accuracy: 0.5747232472324724
At round 25 training accuracy: 0.556875
At round 25 training loss: 1.5497691921765606
gradient difference: 117.73328114537276
At round 26 accuracy: 0.566420664206642
At round 26 training accuracy: 0.5516666666666666
At round 26 training loss: 1.4947917780994127
gradient difference: 116.61721378538205
At round 27 accuracy: 0.514760147601476
At round 27 training accuracy: 0.5076041666666666
At round 27 training loss: 1.6290871258887152
gradient difference: 125.38197436695845
At round 28 accuracy: 0.6079335793357934
At round 28 training accuracy: 0.6321875
At round 28 training loss: 1.4450156836925696
gradient difference: 104.8723511352723
At round 29 accuracy: 0.6033210332103321
At round 29 training accuracy: 0.62625
At round 29 training loss: 1.384157378198579
gradient difference: 105.07824727268653
At round 30 accuracy: 0.6070110701107011
At round 30 training accuracy: 0.6392708333333333
At round 30 training loss: 1.3282281090365722
gradient difference: 102.07562377523858
At round 31 accuracy: 0.6005535055350554
At round 31 training accuracy: 0.6273958333333334
At round 31 training loss: 1.477911716157881
gradient difference: 107.9379111000481
At round 32 accuracy: 0.5968634686346863
At round 32 training accuracy: 0.6254166666666666
At round 32 training loss: 1.5523517258667077
gradient difference: 109.1767148232188
At round 33 accuracy: 0.5802583025830258
At round 33 training accuracy: 0.5882291666666667
At round 33 training loss: 1.6596753108656654
gradient difference: 117.0983259072256
At round 34 accuracy: 0.5793357933579336
At round 34 training accuracy: 0.5845833333333333
At round 34 training loss: 1.6888014578570922
gradient difference: 118.01446168418104
At round 35 accuracy: 0.5885608856088561
At round 35 training accuracy: 0.6119791666666666
At round 35 training loss: 1.6845517012973625
gradient difference: 114.04750926842684
At round 36 accuracy: 0.5239852398523985
At round 36 training accuracy: 0.5086458333333334
At round 36 training loss: 1.8461211034407219
gradient difference: 129.9648413678407
At round 37 accuracy: 0.566420664206642
At round 37 training accuracy: 0.57875
At round 37 training loss: 1.684497115208457
gradient difference: 117.36858271693579
At round 38 accuracy: 0.46309963099630996
At round 38 training accuracy: 0.45979166666666665
At round 38 training loss: 2.2175342585425826
gradient difference: 145.46366761038573
At round 39 accuracy: 0.45940959409594095
At round 39 training accuracy: 0.44875
At round 39 training loss: 2.3249538525845854
gradient difference: 147.64690712799523
At round 40 accuracy: 0.6023985239852399
At round 40 training accuracy: 0.6047916666666666
At round 40 training loss: 1.538854494295083
gradient difference: 111.4096163289601
At round 41 accuracy: 0.5802583025830258
At round 41 training accuracy: 0.5786458333333333
At round 41 training loss: 1.6646051130350679
gradient difference: 117.76000957452915
At round 42 accuracy: 0.5922509225092251
At round 42 training accuracy: 0.5991666666666666
At round 42 training loss: 1.6185952595357473
gradient difference: 114.1395875093311
At round 43 accuracy: 0.6033210332103321
At round 43 training accuracy: 0.636875
At round 43 training loss: 1.5867427583566556
gradient difference: 108.23875164260897
At round 44 accuracy: 0.5987084870848709
At round 44 training accuracy: 0.6117708333333334
At round 44 training loss: 1.5343990921384345
gradient difference: 109.86423002153434
At round 45 accuracy: 0.46955719557195574
At round 45 training accuracy: 0.48635416666666664
At round 45 training loss: 2.0685727152911326
gradient difference: 128.3926069815778
At round 46 accuracy: 0.4797047970479705
At round 46 training accuracy: 0.4998958333333333
At round 46 training loss: 1.9155896537269776
gradient difference: 123.7856502737286
At round 47 accuracy: 0.4732472324723247
At round 47 training accuracy: 0.4996875
At round 47 training loss: 1.9287350668447714
gradient difference: 124.97404415841562
At round 48 accuracy: 0.6171586715867159
At round 48 training accuracy: 0.6525
At round 48 training loss: 1.3570167568527782
gradient difference: 100.90946867112974
At round 49 accuracy: 0.6033210332103321
At round 49 training accuracy: 0.6389583333333333
At round 49 training loss: 1.4320126353250817
gradient difference: 103.1700429979581
At round 50 accuracy: 0.6162361623616236
At round 50 training accuracy: 0.6438541666666666
At round 50 training loss: 1.4584830675382787
gradient difference: 103.60697934611873
At round 51 accuracy: 0.6079335793357934
At round 51 training accuracy: 0.6147916666666666
At round 51 training loss: 1.4622722975552702
gradient difference: 107.66185390326142
At round 52 accuracy: 0.5987084870848709
At round 52 training accuracy: 0.6282291666666666
At round 52 training loss: 1.479393443656154
gradient difference: 104.08429135015955
At round 53 accuracy: 0.6236162361623616
At round 53 training accuracy: 0.6464583333333334
At round 53 training loss: 1.4364939476953198
gradient difference: 103.23763447021568
At round 54 accuracy: 0.6226937269372693
At round 54 training accuracy: 0.6441666666666667
At round 54 training loss: 1.4598966790223493
gradient difference: 104.32217000487115
At round 55 accuracy: 0.6199261992619927
At round 55 training accuracy: 0.6477083333333333
At round 55 training loss: 1.417407666609312
gradient difference: 101.14544584597256
At round 56 accuracy: 0.6171586715867159
At round 56 training accuracy: 0.6377083333333333
At round 56 training loss: 1.3836434362580379
gradient difference: 101.21109565915178
At round 57 accuracy: 0.6180811808118081
At round 57 training accuracy: 0.6246875
At round 57 training loss: 1.3812492925440893
gradient difference: 101.9980735371022
At round 58 accuracy: 0.6273062730627307
At round 58 training accuracy: 0.6292708333333333
At round 58 training loss: 1.3584211032884195
gradient difference: 100.31471203222033
At round 59 accuracy: 0.6328413284132841
At round 59 training accuracy: 0.6639583333333333
At round 59 training loss: 1.3474931439276163
gradient difference: 95.9041622719656
At round 60 accuracy: 0.6402214022140221
At round 60 training accuracy: 0.6670833333333334
At round 60 training loss: 1.2973126868344844
gradient difference: 91.91341568541132
At round 61 accuracy: 0.6171586715867159
At round 61 training accuracy: 0.6102083333333334
At round 61 training loss: 1.372525617987849
gradient difference: 100.61723558096215
At round 62 accuracy: 0.6171586715867159
At round 62 training accuracy: 0.6133333333333333
At round 62 training loss: 1.33572671717188
gradient difference: 97.22982248882043
At round 63 accuracy: 0.6217712177121771
At round 63 training accuracy: 0.6103125
At round 63 training loss: 1.315529537155914
gradient difference: 96.86188011249781
At round 64 accuracy: 0.6429889298892989
At round 64 training accuracy: 0.6510416666666666
At round 64 training loss: 1.2246671945648269
gradient difference: 90.46510853865047
At round 65 accuracy: 0.6420664206642066
At round 65 training accuracy: 0.65375
At round 65 training loss: 1.2634788348323975
gradient difference: 92.20485099531585
At round 66 accuracy: 0.6014760147601476
At round 66 training accuracy: 0.5901041666666667
At round 66 training loss: 1.3836270185451334
gradient difference: 101.95550675541004
At round 67 accuracy: 0.6208487084870848
At round 67 training accuracy: 0.6186458333333333
At round 67 training loss: 1.2720230526166658
gradient difference: 95.67461017632974
At round 68 accuracy: 0.6448339483394834
At round 68 training accuracy: 0.67625
At round 68 training loss: 1.1110216257969538
gradient difference: 83.71322568305604
At round 69 accuracy: 0.6485239852398524
At round 69 training accuracy: 0.6770833333333334
At round 69 training loss: 1.0917618951387704
gradient difference: 82.30625368402556
At round 70 accuracy: 0.6309963099630996
At round 70 training accuracy: 0.6430208333333334
At round 70 training loss: 1.1652696745439122
gradient difference: 88.34768370344078
At round 71 accuracy: 0.6383763837638377
At round 71 training accuracy: 0.6498958333333333
At round 71 training loss: 1.1946400384868805
gradient difference: 89.12612915820952
At round 72 accuracy: 0.6439114391143912
At round 72 training accuracy: 0.6738541666666666
At round 72 training loss: 1.1723742921402056
gradient difference: 85.21093308784394
At round 73 accuracy: 0.6420664206642066
At round 73 training accuracy: 0.6484375
At round 73 training loss: 1.185116847101599
gradient difference: 88.75079020113341
At round 74 accuracy: 0.6116236162361623
At round 74 training accuracy: 0.59625
At round 74 training loss: 1.2909767919188986
gradient difference: 96.94647159608336
At round 75 accuracy: 0.6374538745387454
At round 75 training accuracy: 0.6505208333333333
At round 75 training loss: 1.0961554982342447
gradient difference: 80.61367745985292
At round 76 accuracy: 0.6476014760147601
At round 76 training accuracy: 0.6780208333333333
At round 76 training loss: 1.0902869672436888
gradient difference: 80.77469550409565
At round 77 accuracy: 0.6365313653136532
At round 77 training accuracy: 0.649375
At round 77 training loss: 1.1239094618552674
gradient difference: 81.26811464767957
At round 78 accuracy: 0.6522140221402214
At round 78 training accuracy: 0.6802083333333333
At round 78 training loss: 1.1092783807093898
gradient difference: 80.21840642051144
At round 79 accuracy: 0.6448339483394834
At round 79 training accuracy: 0.6583333333333333
At round 79 training loss: 1.164579372370305
gradient difference: 85.24493458919345
At round 80 accuracy: 0.6328413284132841
At round 80 training accuracy: 0.644375
At round 80 training loss: 1.1705120014042283
gradient difference: 86.50140735686045
At round 81 accuracy: 0.551660516605166
At round 81 training accuracy: 0.5741666666666667
At round 81 training loss: 1.3375133573946854
gradient difference: 91.69875204124422
At round 82 accuracy: 0.6512915129151291
At round 82 training accuracy: 0.6748958333333334
At round 82 training loss: 1.0997490581404419
gradient difference: 80.66051577188621
At round 83 accuracy: 0.6466789667896679
At round 83 training accuracy: 0.6802083333333333
At round 83 training loss: 1.096544887747926
gradient difference: 79.45436762399216
At round 84 accuracy: 0.6485239852398524
At round 84 training accuracy: 0.6804166666666667
At round 84 training loss: 1.095697014022929
gradient difference: 79.25691878565142
At round 85 accuracy: 0.6411439114391144
At round 85 training accuracy: 0.6728125
At round 85 training loss: 1.113544280010586
gradient difference: 79.84811015069148
At round 86 accuracy: 0.6605166051660517
At round 86 training accuracy: 0.6840625
At round 86 training loss: 1.0808513383831209
gradient difference: 74.50785522385841
At round 87 accuracy: 0.6476014760147601
At round 87 training accuracy: 0.6794791666666666
At round 87 training loss: 1.0765514639625327
gradient difference: 73.61983523232644
At round 88 accuracy: 0.6051660516605166
At round 88 training accuracy: 0.6245833333333334
At round 88 training loss: 1.1582129865977913
gradient difference: 80.46307643209757
At round 89 accuracy: 0.6116236162361623
At round 89 training accuracy: 0.6355208333333333
At round 89 training loss: 1.116141919880174
gradient difference: 77.99652420371042
At round 90 accuracy: 0.6337638376383764
At round 90 training accuracy: 0.6509375
At round 90 training loss: 1.0697252101032064
gradient difference: 76.36781633058185
At round 91 accuracy: 0.6245387453874539
At round 91 training accuracy: 0.6434375
At round 91 training loss: 1.0508927330902467
gradient difference: 75.12745079704548
At round 92 accuracy: 0.665129151291513
At round 92 training accuracy: 0.6980208333333333
At round 92 training loss: 0.9570970811508596
gradient difference: 70.170281343979
At round 93 accuracy: 0.6300738007380073
At round 93 training accuracy: 0.62875
At round 93 training loss: 1.0755443318157147
gradient difference: 81.17485415817677
At round 94 accuracy: 0.6485239852398524
At round 94 training accuracy: 0.668125
At round 94 training loss: 0.970923605125087
gradient difference: 72.011544630829
At round 95 accuracy: 0.6494464944649446
At round 95 training accuracy: 0.6417708333333333
At round 95 training loss: 1.027503837638845
gradient difference: 75.3004373832085
At round 96 accuracy: 0.6162361623616236
At round 96 training accuracy: 0.6033333333333334
At round 96 training loss: 1.1419008395820855
gradient difference: 83.39891806396491
At round 97 accuracy: 0.6448339483394834
At round 97 training accuracy: 0.63625
At round 97 training loss: 1.030223448096464
gradient difference: 76.12768235936815
At round 98 accuracy: 0.6660516605166051
At round 98 training accuracy: 0.7036458333333333
At round 98 training loss: 0.9322543704540779
gradient difference: 67.76670661022656
At round 99 accuracy: 0.6503690036900369
At round 99 training accuracy: 0.6879166666666666
At round 99 training loss: 0.9929045633226633
gradient difference: 70.72023145841682
At round 100 accuracy: 0.6688191881918819
At round 100 training accuracy: 0.6925
At round 100 training loss: 0.9945233796909452
gradient difference: 72.00323661748934
At round 101 accuracy: 0.6503690036900369
At round 101 training accuracy: 0.6547916666666667
At round 101 training loss: 1.053485231138766
gradient difference: 77.97364756298815
At round 102 accuracy: 0.6356088560885609
At round 102 training accuracy: 0.6371875
At round 102 training loss: 1.0926568871581306
gradient difference: 80.74148888711936
At round 103 accuracy: 0.6669741697416974
At round 103 training accuracy: 0.6960416666666667
At round 103 training loss: 0.9502952610608191
gradient difference: 70.96788576509957
At round 104 accuracy: 0.6429889298892989
At round 104 training accuracy: 0.650625
At round 104 training loss: 1.0705301176042605
gradient difference: 80.11968003989702
At round 105 accuracy: 0.6595940959409594
At round 105 training accuracy: 0.6983333333333334
At round 105 training loss: 1.0131535834881167
gradient difference: 73.22327233950196
At round 106 accuracy: 0.6669741697416974
At round 106 training accuracy: 0.6939583333333333
At round 106 training loss: 1.0028194818645715
gradient difference: 73.45589061708124
At round 107 accuracy: 0.6466789667896679
At round 107 training accuracy: 0.6546875
At round 107 training loss: 1.0694210513556996
gradient difference: 79.21229739502601
At round 108 accuracy: 0.6605166051660517
At round 108 training accuracy: 0.680625
At round 108 training loss: 0.9841916901478543
gradient difference: 74.8172104284541
At round 109 accuracy: 0.6642066420664207
At round 109 training accuracy: 0.6923958333333333
At round 109 training loss: 0.993347267494537
gradient difference: 74.4395292158015
At round 110 accuracy: 0.6642066420664207
At round 110 training accuracy: 0.6821875
At round 110 training loss: 0.9980819021506856
gradient difference: 74.98635228764964
At round 111 accuracy: 0.5959409594095941
At round 111 training accuracy: 0.5798958333333334
At round 111 training loss: 1.2932018664355078
gradient difference: 92.75083544676323
At round 112 accuracy: 0.6577490774907749
At round 112 training accuracy: 0.6657291666666667
At round 112 training loss: 1.0259573198808356
gradient difference: 76.28839335283216
At round 113 accuracy: 0.6743542435424354
At round 113 training accuracy: 0.6833333333333333
At round 113 training loss: 0.9882113575578356
gradient difference: 73.86573395010858
At round 114 accuracy: 0.6605166051660517
At round 114 training accuracy: 0.6883333333333334
At round 114 training loss: 0.9773857210995629
gradient difference: 70.21912352535004
At round 115 accuracy: 0.6780442804428044
At round 115 training accuracy: 0.7032291666666667
At round 115 training loss: 0.9747901439511527
gradient difference: 71.11372412572914
At round 116 accuracy: 0.672509225092251
At round 116 training accuracy: 0.6958333333333333
At round 116 training loss: 0.9786187449547773
gradient difference: 73.40862630314471
At round 117 accuracy: 0.6697416974169742
At round 117 training accuracy: 0.6935416666666666
At round 117 training loss: 1.0058416553710898
gradient difference: 75.02374154413059
At round 118 accuracy: 0.6392988929889298
At round 118 training accuracy: 0.6244791666666667
At round 118 training loss: 1.1377232585971555
gradient difference: 84.16016638958746
At round 119 accuracy: 0.6153136531365314
At round 119 training accuracy: 0.5989583333333334
At round 119 training loss: 1.208418375273856
gradient difference: 87.72010843594057
At round 120 accuracy: 0.6503690036900369
At round 120 training accuracy: 0.6523958333333333
At round 120 training loss: 1.0453190555376932
gradient difference: 78.01553996081422
At round 121 accuracy: 0.6291512915129152
At round 121 training accuracy: 0.6171875
At round 121 training loss: 1.1500252003967761
gradient difference: 85.57603331997174
At round 122 accuracy: 0.5950184501845018
At round 122 training accuracy: 0.5786458333333333
At round 122 training loss: 1.3061183381670465
gradient difference: 94.22846999979319
At round 123 accuracy: 0.6817343173431735
At round 123 training accuracy: 0.7086458333333333
At round 123 training loss: 0.9162473596616959
gradient difference: 68.87765153261017
At round 124 accuracy: 0.6559040590405905
At round 124 training accuracy: 0.6866666666666666
At round 124 training loss: 0.95009037444368
gradient difference: 71.01842369484484
At round 125 accuracy: 0.6522140221402214
At round 125 training accuracy: 0.668125
At round 125 training loss: 0.986241407917502
gradient difference: 73.12242678037441
At round 126 accuracy: 0.566420664206642
At round 126 training accuracy: 0.5955208333333334
At round 126 training loss: 1.2019327020086348
gradient difference: 84.50323492508764
At round 127 accuracy: 0.5507380073800738
At round 127 training accuracy: 0.5716666666666667
At round 127 training loss: 1.346894822100488
gradient difference: 89.72714053757326
At round 128 accuracy: 0.5138376383763837
At round 128 training accuracy: 0.5389583333333333
At round 128 training loss: 1.7254652534254515
gradient difference: 98.99919302392078
At round 129 accuracy: 0.5018450184501845
At round 129 training accuracy: 0.5305208333333333
At round 129 training loss: 2.381271647171428
gradient difference: 102.52456419345468
At round 130 accuracy: 0.6503690036900369
At round 130 training accuracy: 0.6854166666666667
At round 130 training loss: 0.916318323526842
gradient difference: 67.7093411715951
At round 131 accuracy: 0.672509225092251
At round 131 training accuracy: 0.7016666666666667
At round 131 training loss: 0.8855870695334549
gradient difference: 65.95525072249782
At round 132 accuracy: 0.6697416974169742
At round 132 training accuracy: 0.6940625
At round 132 training loss: 0.9229243019937227
gradient difference: 68.03315957331186
At round 133 accuracy: 0.6595940959409594
At round 133 training accuracy: 0.6666666666666666
At round 133 training loss: 0.9710429020769273
gradient difference: 71.63992415120006
At round 134 accuracy: 0.6697416974169742
At round 134 training accuracy: 0.6877083333333334
At round 134 training loss: 0.9316118844769274
gradient difference: 68.32570321318082
At round 135 accuracy: 0.6743542435424354
At round 135 training accuracy: 0.6954166666666667
At round 135 training loss: 0.9074484717985615
gradient difference: 66.40263453641897
At round 136 accuracy: 0.6217712177121771
At round 136 training accuracy: 0.6507291666666667
At round 136 training loss: 1.0100203375859806
gradient difference: 71.98998340777464
At round 137 accuracy: 0.6134686346863468
At round 137 training accuracy: 0.634375
At round 137 training loss: 1.053153913168547
gradient difference: 73.8844307976333
At round 138 accuracy: 0.6669741697416974
At round 138 training accuracy: 0.678125
At round 138 training loss: 0.9456122076992567
gradient difference: 66.89110980958868
At round 139 accuracy: 0.6494464944649446
At round 139 training accuracy: 0.6622916666666666
At round 139 training loss: 0.9973431605317941
gradient difference: 70.06331423228373
At round 140 accuracy: 0.6660516605166051
At round 140 training accuracy: 0.7071875
At round 140 training loss: 0.9099785460485146
gradient difference: 64.29279552849854
At round 141 accuracy: 0.6208487084870848
At round 141 training accuracy: 0.6346875
At round 141 training loss: 1.0413251166744157
gradient difference: 73.82511660948259
At round 142 accuracy: 0.6346863468634686
At round 142 training accuracy: 0.6605208333333333
At round 142 training loss: 0.9691143236604208
gradient difference: 70.40266375076763
At round 143 accuracy: 0.5599630996309963
At round 143 training accuracy: 0.5782291666666667
At round 143 training loss: 1.3070259802648798
gradient difference: 85.5975507971596
At round 144 accuracy: 0.6263837638376384
At round 144 training accuracy: 0.6394791666666667
At round 144 training loss: 1.0082298512539516
gradient difference: 72.21951028681339
At round 145 accuracy: 0.6955719557195572
At round 145 training accuracy: 0.7196875
At round 145 training loss: 0.8269320430203031
gradient difference: 61.27193462014013
At round 146 accuracy: 0.6946494464944649
At round 146 training accuracy: 0.7242708333333333
At round 146 training loss: 0.8133566627915327
gradient difference: 58.58740259828327
At round 147 accuracy: 0.7001845018450185
At round 147 training accuracy: 0.7405208333333333
At round 147 training loss: 0.8006731722814341
gradient difference: 57.81714723610608
At round 148 accuracy: 0.6531365313653137
At round 148 training accuracy: 0.68
At round 148 training loss: 0.9054511271324008
gradient difference: 64.82031860171543
At round 149 accuracy: 0.7066420664206642
At round 149 training accuracy: 0.7309375
At round 149 training loss: 0.818940527086767
gradient difference: 59.60002567253593
At round 150 accuracy: 0.7038745387453874
At round 150 training accuracy: 0.7307291666666667
At round 150 training loss: 0.8137367571378127
gradient difference: 57.526047841572904
At round 151 accuracy: 0.7020295202952029
At round 151 training accuracy: 0.7308333333333333
At round 151 training loss: 0.8228376785634707
gradient difference: 58.06532977316245
At round 152 accuracy: 0.683579335793358
At round 152 training accuracy: 0.689375
At round 152 training loss: 0.9215376125369221
gradient difference: 66.04875651692812
At round 153 accuracy: 0.6872693726937269
At round 153 training accuracy: 0.7010416666666667
At round 153 training loss: 0.917328466878583
gradient difference: 65.8191526862226
At round 154 accuracy: 0.6918819188191881
At round 154 training accuracy: 0.7128125
At round 154 training loss: 0.907197458196121
gradient difference: 64.97846353832675
At round 155 accuracy: 0.6881918819188192
At round 155 training accuracy: 0.7246875
At round 155 training loss: 0.8678351167558382
gradient difference: 62.20155496446302
At round 156 accuracy: 0.6688191881918819
At round 156 training accuracy: 0.6928125
At round 156 training loss: 0.9353445627912879
gradient difference: 66.1293373375947
At round 157 accuracy: 0.5433579335793358
At round 157 training accuracy: 0.575625
At round 157 training loss: 1.4391530168584237
gradient difference: 87.37712321049686
At round 158 accuracy: 0.6964944649446494
At round 158 training accuracy: 0.72375
At round 158 training loss: 0.8554178961614768
gradient difference: 62.484388764249914
At round 159 accuracy: 0.6955719557195572
At round 159 training accuracy: 0.7245833333333334
At round 159 training loss: 0.8740858337019259
gradient difference: 63.22152739853388
At round 160 accuracy: 0.690959409594096
At round 160 training accuracy: 0.7232291666666667
At round 160 training loss: 0.8537935342624162
gradient difference: 63.411003656185315
At round 161 accuracy: 0.6881918819188192
At round 161 training accuracy: 0.72
At round 161 training loss: 0.853434962159954
gradient difference: 63.43645035900966
At round 162 accuracy: 0.6752767527675276
At round 162 training accuracy: 0.6869791666666667
At round 162 training loss: 0.9039872004805753
gradient difference: 65.86368638755845
At round 163 accuracy: 0.6752767527675276
At round 163 training accuracy: 0.67625
At round 163 training loss: 0.9353128569402421
gradient difference: 67.9869784849421
At round 164 accuracy: 0.6891143911439115
At round 164 training accuracy: 0.7041666666666667
At round 164 training loss: 0.8627290460715691
gradient difference: 62.73846154028561
At round 165 accuracy: 0.690959409594096
At round 165 training accuracy: 0.7221875
At round 165 training loss: 0.8484920821525157
gradient difference: 60.561603162139235
At round 166 accuracy: 0.5867158671586716
At round 166 training accuracy: 0.6130208333333333
At round 166 training loss: 1.1550737974016616
gradient difference: 77.85556292405936
At round 167 accuracy: 0.5738007380073801
At round 167 training accuracy: 0.5957291666666666
At round 167 training loss: 1.2480530602655684
gradient difference: 80.12428455223305
At round 168 accuracy: 0.7038745387453874
At round 168 training accuracy: 0.7304166666666667
At round 168 training loss: 0.8242311963733907
gradient difference: 58.63430894473727
At round 169 accuracy: 0.6928044280442804
At round 169 training accuracy: 0.7248958333333333
At round 169 training loss: 0.8296867135958746
gradient difference: 57.968768719671885
At round 170 accuracy: 0.6946494464944649
At round 170 training accuracy: 0.7254166666666667
At round 170 training loss: 0.8282185471616685
gradient difference: 58.071195132431576
At round 171 accuracy: 0.5479704797047971
At round 171 training accuracy: 0.5733333333333334
At round 171 training loss: 1.4312646600914498
gradient difference: 84.6969059637963
At round 172 accuracy: 0.5488929889298892
At round 172 training accuracy: 0.578125
At round 172 training loss: 1.4054137641160438
gradient difference: 83.55119378619635
At round 173 accuracy: 0.5627306273062731
At round 173 training accuracy: 0.5873958333333333
At round 173 training loss: 1.2999871627055108
gradient difference: 80.6639453055643
At round 174 accuracy: 0.5673431734317343
At round 174 training accuracy: 0.5928125
At round 174 training loss: 1.2866276456803705
gradient difference: 79.90002670303542
At round 175 accuracy: 0.5359778597785978
At round 175 training accuracy: 0.5597916666666667
At round 175 training loss: 1.8887361619838823
gradient difference: 90.52696943238519
At round 176 accuracy: 0.6845018450184502
At round 176 training accuracy: 0.7225
At round 176 training loss: 0.7989595889331153
gradient difference: 54.84873502572921
At round 177 accuracy: 0.6946494464944649
At round 177 training accuracy: 0.7092708333333333
At round 177 training loss: 0.8145606373483315
gradient difference: 54.3673055700683
At round 178 accuracy: 0.6568265682656826
At round 178 training accuracy: 0.6866666666666666
At round 178 training loss: 0.8587319050729275
gradient difference: 58.00494434844942
At round 179 accuracy: 0.6263837638376384
At round 179 training accuracy: 0.6426041666666666
At round 179 training loss: 0.9903805874396736
gradient difference: 65.6214581573981
At round 180 accuracy: 0.5525830258302583
At round 180 training accuracy: 0.5760416666666667
At round 180 training loss: 1.546180018108959
gradient difference: 83.39809992877466
At round 181 accuracy: 0.6374538745387454
At round 181 training accuracy: 0.6536458333333334
At round 181 training loss: 0.9436429358227179
gradient difference: 62.2809752214472
At round 182 accuracy: 0.6346863468634686
At round 182 training accuracy: 0.6547916666666667
At round 182 training loss: 0.9407756693024809
gradient difference: 62.14014748615356
At round 183 accuracy: 0.7121771217712177
At round 183 training accuracy: 0.7451041666666667
At round 183 training loss: 0.7284506706505393
gradient difference: 49.72954424817181
At round 184 accuracy: 0.6955719557195572
At round 184 training accuracy: 0.7202083333333333
At round 184 training loss: 0.7672691013369088
gradient difference: 53.19051566833617
At round 185 accuracy: 0.6854243542435424
At round 185 training accuracy: 0.6940625
At round 185 training loss: 0.8145983098105838
gradient difference: 56.76680239448345
At round 186 accuracy: 0.705719557195572
At round 186 training accuracy: 0.7276041666666667
At round 186 training loss: 0.7458219222646828
gradient difference: 51.07377549520835
At round 187 accuracy: 0.6964944649446494
At round 187 training accuracy: 0.7221875
At round 187 training loss: 0.789068885523205
gradient difference: 54.85673788384223
At round 188 accuracy: 0.698339483394834
At round 188 training accuracy: 0.7170833333333333
At round 188 training loss: 0.7771725393024583
gradient difference: 52.754270059785604
At round 189 accuracy: 0.7066420664206642
At round 189 training accuracy: 0.7435416666666667
At round 189 training loss: 0.7397271494722615
gradient difference: 49.98677379670333
At round 190 accuracy: 0.7075645756457565
At round 190 training accuracy: 0.7445833333333334
At round 190 training loss: 0.7335281900006035
gradient difference: 49.40497532528054
At round 191 accuracy: 0.7029520295202952
At round 191 training accuracy: 0.7345833333333334
At round 191 training loss: 0.7686832189715157
gradient difference: 52.8416359684587
At round 192 accuracy: 0.6854243542435424
At round 192 training accuracy: 0.7270833333333333
At round 192 training loss: 0.7791378448344767
gradient difference: 54.02153121687847
At round 193 accuracy: 0.6955719557195572
At round 193 training accuracy: 0.7136458333333333
At round 193 training loss: 0.8166405712068081
gradient difference: 57.44701765773667
At round 194 accuracy: 0.6512915129151291
At round 194 training accuracy: 0.6780208333333333
At round 194 training loss: 0.918938686788703
gradient difference: 63.038849712798424
At round 195 accuracy: 0.6946494464944649
At round 195 training accuracy: 0.7129166666666666
At round 195 training loss: 0.835432558208704
gradient difference: 58.70282626747981
At round 196 accuracy: 0.6881918819188192
At round 196 training accuracy: 0.7177083333333333
At round 196 training loss: 0.8238441877719015
gradient difference: 56.602358278294595
At round 197 accuracy: 0.6697416974169742
At round 197 training accuracy: 0.6916666666666667
At round 197 training loss: 0.8770089655555785
gradient difference: 61.222604478719425
At round 198 accuracy: 0.6439114391143912
At round 198 training accuracy: 0.6723958333333333
At round 198 training loss: 0.9132883677321175
gradient difference: 63.66203280049984
At round 199 accuracy: 0.7020295202952029
At round 199 training accuracy: 0.7328125
At round 199 training loss: 0.7985668427590281
gradient difference: 57.432159598556254
At round 200 accuracy: 0.6928044280442804
At round 200 training accuracy: 0.7096875
