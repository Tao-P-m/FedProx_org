2022-08-19 10:24:10.746021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
WARNING:tensorflow:From /home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_1_1
	     drop_percent : 0.5
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  warnings.warn('`tf.layers.dense` is deprecated and '
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.
  warnings.warn('`layer.apply` is deprecated and '
2022-08-19 10:24:12.978451: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-08-19 10:24:12.979473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-08-19 10:24:13.364581: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2022-08-19 10:24:13.364628: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node03): /proc/driver/nvidia/version does not exist
2022-08-19 10:24:13.364926: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-19 10:24:13.365020: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-08-19 10:24:13.366845: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)
2022-08-19 10:24:13.368263: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
WARNING:tensorflow:From /home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
Incomplete shape.
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================
Incomplete shape.
Incomplete shape.

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/4.84k flops)
  dense/kernel/Initializer/random_uniform (600/1.20k flops)
    dense/kernel/Initializer/random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/AssignSub (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  dense/kernel/Regularizer/Square (600/600 flops)
  dense/kernel/Regularizer/Sum (599/599 flops)
  PGD/update_dense/bias/AssignSub (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  dense/kernel/Regularizer/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
WARNING:tensorflow:From /home/aig/NailIt/FedProx/flearn/models/synthetic/mclr.py:58: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer Variable.assign which has equivalent behavior in 2.X.
At round 0 accuracy: 0.035977859778597784
At round 0 training accuracy: 0.035104166666666665
At round 0 training loss: 4.849575695159535
gradient difference: 146.9289105305644
At round 1 accuracy: 0.09040590405904059
At round 1 training accuracy: 0.0934375
At round 1 training loss: 5.266720902894934
gradient difference: 166.93778972534818
At round 2 accuracy: 0.09778597785977859
At round 2 training accuracy: 0.1046875
At round 2 training loss: 5.526724230833351
gradient difference: 166.33821923749977
At round 3 accuracy: 0.11346863468634687
At round 3 training accuracy: 0.1184375
At round 3 training loss: 5.0242274323229985
gradient difference: 162.24588664387176
At round 4 accuracy: 0.48616236162361626
At round 4 training accuracy: 0.5160416666666666
At round 4 training loss: 1.5851261856344838
gradient difference: 94.70605540489393
At round 5 accuracy: 0.4907749077490775
At round 5 training accuracy: 0.48260416666666667
At round 5 training loss: 2.053704161072771
gradient difference: 112.75172472625773
At round 6 accuracy: 0.5119926199261993
At round 6 training accuracy: 0.5502083333333333
At round 6 training loss: 1.7592767320356022
gradient difference: 103.7186107988396
At round 7 accuracy: 0.5433579335793358
At round 7 training accuracy: 0.5402083333333333
At round 7 training loss: 1.4739862325042485
gradient difference: 101.3586086414941
At round 8 accuracy: 0.29704797047970477
At round 8 training accuracy: 0.3078125
At round 8 training loss: 1.7013699616243443
gradient difference: 128.31165477900953
At round 9 accuracy: 0.5378228782287823
At round 9 training accuracy: 0.5596875
At round 9 training loss: 1.6509298397259167
gradient difference: 107.4069055975918
At round 10 accuracy: 0.5738007380073801
At round 10 training accuracy: 0.5973958333333333
At round 10 training loss: 1.460720667419955
gradient difference: 102.75121201704606
At round 11 accuracy: 0.5987084870848709
At round 11 training accuracy: 0.6302083333333334
At round 11 training loss: 1.3694782899972051
gradient difference: 98.54402447425831
At round 12 accuracy: 0.5867158671586716
At round 12 training accuracy: 0.6186458333333333
At round 12 training loss: 1.5347588526954254
gradient difference: 104.11417278176184
At round 13 accuracy: 0.522140221402214
At round 13 training accuracy: 0.5547916666666667
At round 13 training loss: 1.8063034121505916
gradient difference: 115.0305472558957
At round 14 accuracy: 0.5424354243542435
At round 14 training accuracy: 0.5719791666666667
At round 14 training loss: 1.8966193893676002
gradient difference: 115.45504694706986
At round 15 accuracy: 0.5654981549815498
At round 15 training accuracy: 0.6028125
At round 15 training loss: 1.87333257811144
gradient difference: 113.8498224485257
At round 16 accuracy: 0.45571955719557194
At round 16 training accuracy: 0.48520833333333335
At round 16 training loss: 2.107754259379581
gradient difference: 128.61344239869254
At round 17 accuracy: 0.4437269372693727
At round 17 training accuracy: 0.46270833333333333
At round 17 training loss: 2.0999587634190298
gradient difference: 129.88682120241745
At round 18 accuracy: 0.4547970479704797
At round 18 training accuracy: 0.4791666666666667
At round 18 training loss: 1.9480848625069485
gradient difference: 125.01659526928644
At round 19 accuracy: 0.43911439114391143
At round 19 training accuracy: 0.45875
At round 19 training loss: 2.0058098578685897
gradient difference: 127.76214517941864
At round 20 accuracy: 0.4566420664206642
At round 20 training accuracy: 0.475
At round 20 training loss: 1.8200373628285402
gradient difference: 122.53899677466461
At round 21 accuracy: 0.49907749077490776
At round 21 training accuracy: 0.5202083333333334
At round 21 training loss: 1.6469044259143992
gradient difference: 115.33624278300218
At round 22 accuracy: 0.4474169741697417
At round 22 training accuracy: 0.469375
At round 22 training loss: 1.9888178906279306
gradient difference: 124.37894063143995
At round 23 accuracy: 0.5802583025830258
At round 23 training accuracy: 0.6010416666666667
At round 23 training loss: 1.5183512712223455
gradient difference: 110.28058603658981
At round 24 accuracy: 0.5608856088560885
At round 24 training accuracy: 0.578125
At round 24 training loss: 1.6304608725756407
gradient difference: 114.25478523910043
At round 25 accuracy: 0.6107011070110702
At round 25 training accuracy: 0.6361458333333333
At round 25 training loss: 1.443639084440656
gradient difference: 105.92593450982623
At round 26 accuracy: 0.5987084870848709
At round 26 training accuracy: 0.6357291666666667
At round 26 training loss: 1.3759296431050947
gradient difference: 103.37194890648053
At round 27 accuracy: 0.6023985239852399
At round 27 training accuracy: 0.6202083333333334
At round 27 training loss: 1.373343750438653
gradient difference: 105.12416358362725
At round 28 accuracy: 0.5811808118081181
At round 28 training accuracy: 0.5815625
At round 28 training loss: 1.57481281022386
gradient difference: 114.8575369464061
At round 29 accuracy: 0.5636531365313653
At round 29 training accuracy: 0.555
At round 29 training loss: 1.5811269851882632
gradient difference: 118.45183628812391
At round 30 accuracy: 0.5867158671586716
At round 30 training accuracy: 0.59
At round 30 training loss: 1.4483109664249545
gradient difference: 110.63997590082026
At round 31 accuracy: 0.5950184501845018
At round 31 training accuracy: 0.613125
At round 31 training loss: 1.5182215039525182
gradient difference: 110.40075152617695
At round 32 accuracy: 0.5968634686346863
At round 32 training accuracy: 0.6275
At round 32 training loss: 1.5657870254401738
gradient difference: 108.79847815187615
At round 33 accuracy: 0.5977859778597786
At round 33 training accuracy: 0.6233333333333333
At round 33 training loss: 1.6072480337570112
gradient difference: 110.56813226407553
At round 34 accuracy: 0.6014760147601476
At round 34 training accuracy: 0.6216666666666667
At round 34 training loss: 1.6323660064317906
gradient difference: 111.409359626402
At round 35 accuracy: 0.5940959409594095
At round 35 training accuracy: 0.62125
At round 35 training loss: 1.6725785429310054
gradient difference: 111.59424030538193
At round 36 accuracy: 0.5479704797047971
At round 36 training accuracy: 0.5339583333333333
At round 36 training loss: 1.7550052999674033
gradient difference: 123.46329284860279
At round 37 accuracy: 0.5876383763837638
At round 37 training accuracy: 0.6161458333333333
At round 37 training loss: 1.6132233929075301
gradient difference: 110.42634764034015
At round 38 accuracy: 0.49538745387453875
At round 38 training accuracy: 0.4820833333333333
At round 38 training loss: 2.034217213600253
gradient difference: 138.00488230004132
At round 39 accuracy: 0.470479704797048
At round 39 training accuracy: 0.46677083333333336
At round 39 training loss: 2.1493700691716127
gradient difference: 141.71790087898617
At round 40 accuracy: 0.6051660516605166
At round 40 training accuracy: 0.6386458333333334
At round 40 training loss: 1.5415963652993863
gradient difference: 105.59980511525751
At round 41 accuracy: 0.6051660516605166
At round 41 training accuracy: 0.6379166666666667
At round 41 training loss: 1.5821655438203985
gradient difference: 107.37426188171357
At round 42 accuracy: 0.5913284132841329
At round 42 training accuracy: 0.625625
At round 42 training loss: 1.6072202866928031
gradient difference: 107.92139703952759
At round 43 accuracy: 0.6023985239852399
At round 43 training accuracy: 0.6321875
At round 43 training loss: 1.6104505511947598
gradient difference: 107.98864547943099
At round 44 accuracy: 0.6051660516605166
At round 44 training accuracy: 0.6183333333333333
At round 44 training loss: 1.5371750620457654
gradient difference: 108.52111146918006
At round 45 accuracy: 0.46309963099630996
At round 45 training accuracy: 0.47572916666666665
At round 45 training loss: 2.248780231901134
gradient difference: 131.43609629255013
At round 46 accuracy: 0.46309963099630996
At round 46 training accuracy: 0.4871875
At round 46 training loss: 2.0863755749771373
gradient difference: 127.31194820023696
At round 47 accuracy: 0.46309963099630996
At round 47 training accuracy: 0.4853125
At round 47 training loss: 2.1441712887817994
gradient difference: 128.85329016790172
At round 48 accuracy: 0.6134686346863468
At round 48 training accuracy: 0.6313541666666667
At round 48 training loss: 1.4008600809130196
gradient difference: 105.0131070215072
At round 49 accuracy: 0.6171586715867159
At round 49 training accuracy: 0.6510416666666666
At round 49 training loss: 1.4162198472737024
gradient difference: 102.3734748564419
At round 50 accuracy: 0.5894833948339483
At round 50 training accuracy: 0.5801041666666666
At round 50 training loss: 1.5990917730967824
gradient difference: 116.27055980743259
At round 51 accuracy: 0.5175276752767528
At round 51 training accuracy: 0.5092708333333333
At round 51 training loss: 1.8362806376504401
gradient difference: 129.7124371833127
At round 52 accuracy: 0.5940959409594095
At round 52 training accuracy: 0.6125
At round 52 training loss: 1.5234288248963033
gradient difference: 105.32800457314231
At round 53 accuracy: 0.6180811808118081
At round 53 training accuracy: 0.6435416666666667
At round 53 training loss: 1.4747246540120493
gradient difference: 104.39771230470411
At round 54 accuracy: 0.6070110701107011
At round 54 training accuracy: 0.6380208333333334
At round 54 training loss: 1.5080131338816136
gradient difference: 103.79314328690867
At round 55 accuracy: 0.5894833948339483
At round 55 training accuracy: 0.6094791666666667
At round 55 training loss: 1.523568527669025
gradient difference: 104.99418530755274
At round 56 accuracy: 0.6208487084870848
At round 56 training accuracy: 0.6573958333333333
At round 56 training loss: 1.370123209001807
gradient difference: 97.53781679594044
At round 57 accuracy: 0.6309963099630996
At round 57 training accuracy: 0.6590625
At round 57 training loss: 1.3319956759937728
gradient difference: 95.92099846238702
At round 58 accuracy: 0.6374538745387454
At round 58 training accuracy: 0.6635416666666667
At round 58 training loss: 1.3071079587455219
gradient difference: 94.20831164540999
At round 59 accuracy: 0.5654981549815498
At round 59 training accuracy: 0.5917708333333334
At round 59 training loss: 1.5248937455766525
gradient difference: 103.94562401042457
At round 60 accuracy: 0.6014760147601476
At round 60 training accuracy: 0.6169791666666666
At round 60 training loss: 1.4186249598270904
gradient difference: 97.10820736686595
At round 61 accuracy: 0.6494464944649446
At round 61 training accuracy: 0.6688541666666666
At round 61 training loss: 1.270323097590978
gradient difference: 91.2423513695017
At round 62 accuracy: 0.6476014760147601
At round 62 training accuracy: 0.6708333333333333
At round 62 training loss: 1.2295623203242818
gradient difference: 86.97249133036216
At round 63 accuracy: 0.6494464944649446
At round 63 training accuracy: 0.6723958333333333
At round 63 training loss: 1.1923663862344498
gradient difference: 85.73950928824246
At round 64 accuracy: 0.6494464944649446
At round 64 training accuracy: 0.6694791666666666
At round 64 training loss: 1.1978728852265825
gradient difference: 87.00881902228844
At round 65 accuracy: 0.6466789667896679
At round 65 training accuracy: 0.6588541666666666
At round 65 training loss: 1.259401384422866
gradient difference: 90.99953053155077
At round 66 accuracy: 0.6291512915129152
At round 66 training accuracy: 0.6302083333333334
At round 66 training loss: 1.2753167675994337
gradient difference: 93.69583969641833
At round 67 accuracy: 0.6439114391143912
At round 67 training accuracy: 0.6711458333333333
At round 67 training loss: 1.17128335029042
gradient difference: 87.13031428330352
At round 68 accuracy: 0.6328413284132841
At round 68 training accuracy: 0.6628125
At round 68 training loss: 1.1374858670278143
gradient difference: 83.98481918958696
At round 69 accuracy: 0.6300738007380073
At round 69 training accuracy: 0.6648958333333334
At round 69 training loss: 1.1162092235280823
gradient difference: 82.48426504763275
At round 70 accuracy: 0.6392988929889298
At round 70 training accuracy: 0.656875
At round 70 training loss: 1.1874965458006288
gradient difference: 87.14415565730505
At round 71 accuracy: 0.6365313653136532
At round 71 training accuracy: 0.6728125
At round 71 training loss: 1.1935656665172427
gradient difference: 85.70267537146975
At round 72 accuracy: 0.6374538745387454
At round 72 training accuracy: 0.6704166666666667
At round 72 training loss: 1.2195253101410344
gradient difference: 87.20207669536377
At round 73 accuracy: 0.6346863468634686
At round 73 training accuracy: 0.6454166666666666
At round 73 training loss: 1.2314705099227528
gradient difference: 90.60256539074902
At round 74 accuracy: 0.6060885608856088
At round 74 training accuracy: 0.5939583333333334
At round 74 training loss: 1.340658621015027
gradient difference: 98.87216731136387
At round 75 accuracy: 0.6208487084870848
At round 75 training accuracy: 0.6338541666666667
At round 75 training loss: 1.1684444535399476
gradient difference: 84.18022970288253
At round 76 accuracy: 0.6402214022140221
At round 76 training accuracy: 0.6690625
At round 76 training loss: 1.1294167219568043
gradient difference: 81.96436915316525
At round 77 accuracy: 0.6023985239852399
At round 77 training accuracy: 0.62375
At round 77 training loss: 1.202037144084461
gradient difference: 85.19747712088204
At round 78 accuracy: 0.6512915129151291
At round 78 training accuracy: 0.6727083333333334
At round 78 training loss: 1.1383445720933378
gradient difference: 82.47065583478887
At round 79 accuracy: 0.6439114391143912
At round 79 training accuracy: 0.6717708333333333
At round 79 training loss: 1.1662604467989877
gradient difference: 84.02437145462508
At round 80 accuracy: 0.6402214022140221
At round 80 training accuracy: 0.66125
At round 80 training loss: 1.1578397127644469
gradient difference: 84.5454668289665
At round 81 accuracy: 0.5175276752767528
At round 81 training accuracy: 0.5435416666666667
At round 81 training loss: 1.5241233829703802
gradient difference: 99.47780037823699
At round 82 accuracy: 0.6457564575645757
At round 82 training accuracy: 0.6691666666666667
At round 82 training loss: 1.1270204636625325
gradient difference: 82.97444393906126
At round 83 accuracy: 0.6448339483394834
At round 83 training accuracy: 0.6761458333333333
At round 83 training loss: 1.11785320049307
gradient difference: 80.12434457078258
At round 84 accuracy: 0.6420664206642066
At round 84 training accuracy: 0.6689583333333333
At round 84 training loss: 1.1465935671282932
gradient difference: 83.49814656672028
At round 85 accuracy: 0.6143911439114391
At round 85 training accuracy: 0.6353125
At round 85 training loss: 1.2227159081368397
gradient difference: 85.1002749332789
At round 86 accuracy: 0.6356088560885609
At round 86 training accuracy: 0.65125
At round 86 training loss: 1.178862897308233
gradient difference: 79.10992855382273
At round 87 accuracy: 0.6383763837638377
At round 87 training accuracy: 0.6504166666666666
At round 87 training loss: 1.1669542559034503
gradient difference: 78.81502847257734
At round 88 accuracy: 0.5507380073800738
At round 88 training accuracy: 0.5765625
At round 88 training loss: 1.3517850834550336
gradient difference: 90.05688009923834
At round 89 accuracy: 0.5784132841328413
At round 89 training accuracy: 0.5901041666666667
At round 89 training loss: 1.279384149420075
gradient difference: 86.77197692240355
At round 90 accuracy: 0.5913284132841329
At round 90 training accuracy: 0.6121875
At round 90 training loss: 1.201727530187927
gradient difference: 84.2849235959716
At round 91 accuracy: 0.5839483394833949
At round 91 training accuracy: 0.6044791666666667
At round 91 training loss: 1.1781860350739832
gradient difference: 82.84372946844611
At round 92 accuracy: 0.6632841328413284
At round 92 training accuracy: 0.6953125
At round 92 training loss: 0.9763954996488368
gradient difference: 71.68837302986962
At round 93 accuracy: 0.6328413284132841
At round 93 training accuracy: 0.6370833333333333
At round 93 training loss: 1.0806547490134835
gradient difference: 81.8284909634753
At round 94 accuracy: 0.6531365313653137
At round 94 training accuracy: 0.666875
At round 94 training loss: 0.995665632042413
gradient difference: 73.89709169942486
At round 95 accuracy: 0.6595940959409594
At round 95 training accuracy: 0.6590625
At round 95 training loss: 0.9993131938825051
gradient difference: 73.41845119000689
At round 96 accuracy: 0.6309963099630996
At round 96 training accuracy: 0.6154166666666666
At round 96 training loss: 1.1179007217443238
gradient difference: 81.5310408976139
At round 97 accuracy: 0.6531365313653137
At round 97 training accuracy: 0.65125
At round 97 training loss: 1.0159357780963183
gradient difference: 74.14617128589194
At round 98 accuracy: 0.6743542435424354
At round 98 training accuracy: 0.7032291666666667
At round 98 training loss: 0.9439657186996192
gradient difference: 68.14040919923615
At round 99 accuracy: 0.6688191881918819
At round 99 training accuracy: 0.6865625
At round 99 training loss: 1.0017980253634353
gradient difference: 72.548400580291
At round 100 accuracy: 0.6595940959409594
At round 100 training accuracy: 0.6679166666666667
At round 100 training loss: 1.0499196236332258
gradient difference: 76.23994546004229
At round 101 accuracy: 0.6392988929889298
At round 101 training accuracy: 0.6326041666666666
At round 101 training loss: 1.1264918238048751
gradient difference: 82.562102852592
At round 102 accuracy: 0.6171586715867159
At round 102 training accuracy: 0.6094791666666667
At round 102 training loss: 1.1917577973473816
gradient difference: 86.53642516933286
At round 103 accuracy: 0.6595940959409594
At round 103 training accuracy: 0.6847916666666667
At round 103 training loss: 0.9813537974127878
gradient difference: 73.10419511180594
At round 104 accuracy: 0.6688191881918819
At round 104 training accuracy: 0.6967708333333333
At round 104 training loss: 0.9934039422745506
gradient difference: 72.99640997375711
At round 105 accuracy: 0.6577490774907749
At round 105 training accuracy: 0.6715625
At round 105 training loss: 1.0673399016385277
gradient difference: 78.4925762104237
At round 106 accuracy: 0.6374538745387454
At round 106 training accuracy: 0.6280208333333334
At round 106 training loss: 1.1538885081280024
gradient difference: 84.90276296187024
At round 107 accuracy: 0.6512915129151291
At round 107 training accuracy: 0.6636458333333334
At round 107 training loss: 1.0883266931120306
gradient difference: 79.85024603992328
At round 108 accuracy: 0.665129151291513
At round 108 training accuracy: 0.6883333333333334
At round 108 training loss: 1.0079964265134185
gradient difference: 76.04595758251186
At round 109 accuracy: 0.6503690036900369
At round 109 training accuracy: 0.6858333333333333
At round 109 training loss: 1.0469311000903447
gradient difference: 76.34254728103153
At round 110 accuracy: 0.6605166051660517
At round 110 training accuracy: 0.693125
At round 110 training loss: 1.0122888839337976
gradient difference: 74.26078811186207
At round 111 accuracy: 0.6217712177121771
At round 111 training accuracy: 0.6142708333333333
At round 111 training loss: 1.1796063611765082
gradient difference: 86.40468750114998
At round 112 accuracy: 0.6586715867158671
At round 112 training accuracy: 0.6716666666666666
At round 112 training loss: 1.0345727122512955
gradient difference: 76.46267381943703
At round 113 accuracy: 0.6678966789667896
At round 113 training accuracy: 0.6805208333333334
At round 113 training loss: 1.014255747354279
gradient difference: 75.50668926819652
At round 114 accuracy: 0.6540590405904059
At round 114 training accuracy: 0.6767708333333333
At round 114 training loss: 1.0212455740260582
gradient difference: 72.70189469610251
At round 115 accuracy: 0.672509225092251
At round 115 training accuracy: 0.695625
At round 115 training loss: 1.0025950266063834
gradient difference: 71.80687742802496
At round 116 accuracy: 0.6715867158671587
At round 116 training accuracy: 0.7015625
At round 116 training loss: 0.9857536209902416
gradient difference: 72.69810110085615
At round 117 accuracy: 0.6743542435424354
At round 117 training accuracy: 0.6955208333333334
At round 117 training loss: 1.0194957904036468
gradient difference: 75.48670754632107
At round 118 accuracy: 0.6392988929889298
At round 118 training accuracy: 0.6353125
At round 118 training loss: 1.1223656920902432
gradient difference: 82.25327228921418
At round 119 accuracy: 0.6402214022140221
At round 119 training accuracy: 0.6236458333333333
At round 119 training loss: 1.1472207100999852
gradient difference: 83.25727306008245
At round 120 accuracy: 0.6595940959409594
At round 120 training accuracy: 0.6735416666666667
At round 120 training loss: 1.0211148072158296
gradient difference: 75.35178343043916
At round 121 accuracy: 0.6356088560885609
At round 121 training accuracy: 0.6359375
At round 121 training loss: 1.1066809329312917
gradient difference: 82.26217404527948
At round 122 accuracy: 0.5940959409594095
At round 122 training accuracy: 0.5883333333333334
At round 122 training loss: 1.278327891623291
gradient difference: 92.74342359605059
At round 123 accuracy: 0.6494464944649446
At round 123 training accuracy: 0.6944791666666666
At round 123 training loss: 0.948579436877432
gradient difference: 70.8547642097882
At round 124 accuracy: 0.6319188191881919
At round 124 training accuracy: 0.6578125
At round 124 training loss: 1.021218186384067
gradient difference: 75.558175746841
At round 125 accuracy: 0.6402214022140221
At round 125 training accuracy: 0.6596875
At round 125 training loss: 1.0103786890146633
gradient difference: 74.74227204969199
At round 126 accuracy: 0.5793357933579336
At round 126 training accuracy: 0.6044791666666667
At round 126 training loss: 1.149351744398785
gradient difference: 82.546880144243
At round 127 accuracy: 0.5424354243542435
At round 127 training accuracy: 0.5730208333333333
At round 127 training loss: 1.3247262752708047
gradient difference: 89.19811277782883
At round 128 accuracy: 0.5101476014760148
At round 128 training accuracy: 0.5389583333333333
At round 128 training loss: 1.7178755915599564
gradient difference: 99.5674855192606
At round 129 accuracy: 0.5009225092250923
At round 129 training accuracy: 0.530625
At round 129 training loss: 2.188679100763984
gradient difference: 102.7134553136409
At round 130 accuracy: 0.6485239852398524
At round 130 training accuracy: 0.6761458333333333
At round 130 training loss: 0.9447325468296185
gradient difference: 69.54418620326425
At round 131 accuracy: 0.6531365313653137
At round 131 training accuracy: 0.6955208333333334
At round 131 training loss: 0.9072456728713587
gradient difference: 67.23552755856915
At round 132 accuracy: 0.6595940959409594
At round 132 training accuracy: 0.6985416666666666
At round 132 training loss: 0.9303861366631463
gradient difference: 68.57869124136593
At round 133 accuracy: 0.6503690036900369
At round 133 training accuracy: 0.664375
At round 133 training loss: 0.9880384931325291
gradient difference: 72.49519954942782
At round 134 accuracy: 0.665129151291513
At round 134 training accuracy: 0.6882291666666667
At round 134 training loss: 0.9419041884193817
gradient difference: 68.55978953762015
At round 135 accuracy: 0.6752767527675276
At round 135 training accuracy: 0.7069791666666667
At round 135 training loss: 0.8968055344124635
gradient difference: 64.79651833141857
At round 136 accuracy: 0.6531365313653137
At round 136 training accuracy: 0.6570833333333334
At round 136 training loss: 1.0166211404946321
gradient difference: 74.31003292182977
At round 137 accuracy: 0.6826568265682657
At round 137 training accuracy: 0.7038541666666667
At round 137 training loss: 0.9140305876033381
gradient difference: 65.80666820457395
At round 138 accuracy: 0.6826568265682657
At round 138 training accuracy: 0.6995833333333333
At round 138 training loss: 0.9182012868082772
gradient difference: 65.8369242795949
At round 139 accuracy: 0.6780442804428044
At round 139 training accuracy: 0.7027083333333334
At round 139 training loss: 0.9332967509546628
gradient difference: 66.69423736558576
At round 140 accuracy: 0.6761992619926199
At round 140 training accuracy: 0.68875
At round 140 training loss: 0.9544092141619573
gradient difference: 68.10115154210145
At round 141 accuracy: 0.6688191881918819
At round 141 training accuracy: 0.7044791666666667
At round 141 training loss: 0.8844454097018267
gradient difference: 64.20490668975444
At round 142 accuracy: 0.6808118081180812
At round 142 training accuracy: 0.7155208333333334
At round 142 training loss: 0.8681510207988322
gradient difference: 63.60649173216015
At round 143 accuracy: 0.6143911439114391
At round 143 training accuracy: 0.6341666666666667
At round 143 training loss: 1.036862101011599
gradient difference: 73.78281965198555
At round 144 accuracy: 0.6780442804428044
At round 144 training accuracy: 0.7042708333333333
At round 144 training loss: 0.8554803894972429
gradient difference: 62.732282380384916
At round 145 accuracy: 0.6761992619926199
At round 145 training accuracy: 0.7098958333333333
At round 145 training loss: 0.8577354811908057
gradient difference: 62.7679628003757
At round 146 accuracy: 0.683579335793358
At round 146 training accuracy: 0.7163541666666666
At round 146 training loss: 0.8429002596189579
gradient difference: 59.749975342593224
At round 147 accuracy: 0.7029520295202952
At round 147 training accuracy: 0.7351041666666667
At round 147 training loss: 0.8190428623339782
gradient difference: 58.42444066740363
At round 148 accuracy: 0.6346863468634686
At round 148 training accuracy: 0.6597916666666667
At round 148 training loss: 0.9632050162941839
gradient difference: 68.52160402039254
At round 149 accuracy: 0.6918819188191881
At round 149 training accuracy: 0.7217708333333334
At round 149 training loss: 0.8437077697273344
gradient difference: 60.98579526071826
At round 150 accuracy: 0.6955719557195572
At round 150 training accuracy: 0.7308333333333333
At round 150 training loss: 0.8251013408352932
gradient difference: 58.7250572821949
At round 151 accuracy: 0.6918819188191881
At round 151 training accuracy: 0.7267708333333334
At round 151 training loss: 0.8351641841450085
gradient difference: 59.52552724815143
At round 152 accuracy: 0.6688191881918819
At round 152 training accuracy: 0.6955208333333334
At round 152 training loss: 0.9198194832168519
gradient difference: 64.72705951141944
At round 153 accuracy: 0.6863468634686347
At round 153 training accuracy: 0.7207291666666666
At round 153 training loss: 0.8948052362275
gradient difference: 63.466366584167524
At round 154 accuracy: 0.6845018450184502
At round 154 training accuracy: 0.7111458333333334
At round 154 training loss: 0.927991063358883
gradient difference: 66.67150892455854
At round 155 accuracy: 0.6881918819188192
At round 155 training accuracy: 0.720625
At round 155 training loss: 0.8938568860323479
gradient difference: 64.01008342253976
At round 156 accuracy: 0.6761992619926199
At round 156 training accuracy: 0.7094791666666667
At round 156 training loss: 0.9203797872116168
gradient difference: 65.58635789347629
At round 157 accuracy: 0.559040590405904
At round 157 training accuracy: 0.5866666666666667
At round 157 training loss: 1.360491978538533
gradient difference: 85.8843833285104
At round 158 accuracy: 0.6734317343173432
At round 158 training accuracy: 0.6951041666666666
At round 158 training loss: 0.9317791412289564
gradient difference: 66.4264196432957
At round 159 accuracy: 0.6715867158671587
At round 159 training accuracy: 0.6780208333333333
At round 159 training loss: 0.9765951331363371
gradient difference: 71.55650585856041
At round 160 accuracy: 0.6798892988929889
At round 160 training accuracy: 0.6977083333333334
At round 160 training loss: 0.9127031450749685
gradient difference: 67.96649922027173
At round 161 accuracy: 0.6688191881918819
At round 161 training accuracy: 0.6682291666666667
At round 161 training loss: 0.9667071122427782
gradient difference: 71.58988128613095
At round 162 accuracy: 0.6309963099630996
At round 162 training accuracy: 0.6183333333333333
At round 162 training loss: 1.1198032024906328
gradient difference: 79.77263787189834
At round 163 accuracy: 0.6845018450184502
At round 163 training accuracy: 0.7186458333333333
At round 163 training loss: 0.8672080084774643
gradient difference: 61.9647315332102
At round 164 accuracy: 0.6752767527675276
At round 164 training accuracy: 0.7115625
At round 164 training loss: 0.8657452858425677
gradient difference: 61.7468999667365
At round 165 accuracy: 0.6928044280442804
At round 165 training accuracy: 0.7295833333333334
At round 165 training loss: 0.8505309587468703
gradient difference: 61.001752642247155
At round 166 accuracy: 0.6088560885608856
At round 166 training accuracy: 0.6344791666666667
At round 166 training loss: 1.0673457325296476
gradient difference: 73.96477284949903
At round 167 accuracy: 0.5719557195571956
At round 167 training accuracy: 0.6042708333333333
At round 167 training loss: 1.2097197993875792
gradient difference: 79.35593227925227
At round 168 accuracy: 0.6974169741697417
At round 168 training accuracy: 0.7260416666666667
At round 168 training loss: 0.8424668734598284
gradient difference: 60.33315100430901
At round 169 accuracy: 0.6946494464944649
At round 169 training accuracy: 0.7290625
At round 169 training loss: 0.8348751259579633
gradient difference: 58.4731034650653
At round 170 accuracy: 0.6918819188191881
At round 170 training accuracy: 0.729375
At round 170 training loss: 0.8340513771461944
gradient difference: 58.69922044978569
At round 171 accuracy: 0.5507380073800738
At round 171 training accuracy: 0.5751041666666666
At round 171 training loss: 1.3878485070137927
gradient difference: 84.58748953859744
At round 172 accuracy: 0.5470479704797048
At round 172 training accuracy: 0.5729166666666666
At round 172 training loss: 1.4090237387502567
gradient difference: 85.29643993925272
At round 173 accuracy: 0.5507380073800738
At round 173 training accuracy: 0.5764583333333333
At round 173 training loss: 1.3398964374512434
gradient difference: 83.50651564707496
At round 174 accuracy: 0.5461254612546126
At round 174 training accuracy: 0.5710416666666667
At round 174 training loss: 1.4027490528052051
gradient difference: 85.19131935574083
At round 175 accuracy: 0.5193726937269373
At round 175 training accuracy: 0.5496875
At round 175 training loss: 2.0063192661339415
gradient difference: 94.45129898752212
At round 176 accuracy: 0.6946494464944649
At round 176 training accuracy: 0.7170833333333333
At round 176 training loss: 0.8179717154831936
gradient difference: 57.607547487846794
At round 177 accuracy: 0.6955719557195572
At round 177 training accuracy: 0.733125
At round 177 training loss: 0.7792283697007224
gradient difference: 53.130658701990036
At round 178 accuracy: 0.683579335793358
At round 178 training accuracy: 0.7267708333333334
At round 178 training loss: 0.7792218053371956
gradient difference: 53.497063917905045
At round 179 accuracy: 0.6522140221402214
At round 179 training accuracy: 0.6766666666666666
At round 179 training loss: 0.8715153737459331
gradient difference: 60.14372335553633
At round 180 accuracy: 0.5553505535055351
At round 180 training accuracy: 0.5758333333333333
At round 180 training loss: 1.3793102388083935
gradient difference: 81.54889918835244
At round 181 accuracy: 0.6559040590405905
At round 181 training accuracy: 0.6780208333333333
At round 181 training loss: 0.8579090086153398
gradient difference: 58.626355069265415
At round 182 accuracy: 0.6512915129151291
At round 182 training accuracy: 0.6764583333333334
At round 182 training loss: 0.8636391044439127
gradient difference: 58.756792367047545
At round 183 accuracy: 0.6928044280442804
At round 183 training accuracy: 0.7315625
At round 183 training loss: 0.7884763675695285
gradient difference: 55.36017767918994
At round 184 accuracy: 0.690959409594096
At round 184 training accuracy: 0.7085416666666666
At round 184 training loss: 0.8230545936276515
gradient difference: 58.3359289818582
At round 185 accuracy: 0.6734317343173432
At round 185 training accuracy: 0.6789583333333333
At round 185 training loss: 0.876225751657039
gradient difference: 61.91629413161069
At round 186 accuracy: 0.698339483394834
At round 186 training accuracy: 0.720625
At round 186 training loss: 0.7971173170146842
gradient difference: 55.14862363174726
At round 187 accuracy: 0.683579335793358
At round 187 training accuracy: 0.71625
At round 187 training loss: 0.8346014800869549
gradient difference: 56.97811484696115
At round 188 accuracy: 0.7066420664206642
At round 188 training accuracy: 0.7409375
At round 188 training loss: 0.7637441284271578
gradient difference: 51.71000018639658
At round 189 accuracy: 0.7066420664206642
At round 189 training accuracy: 0.7379166666666667
At round 189 training loss: 0.7974026285670698
gradient difference: 54.67130150389467
At round 190 accuracy: 0.698339483394834
At round 190 training accuracy: 0.733125
At round 190 training loss: 0.7936308883440991
gradient difference: 54.339230097951685
At round 191 accuracy: 0.6817343173431735
At round 191 training accuracy: 0.7104166666666667
At round 191 training loss: 0.857791686328128
gradient difference: 58.77276082343446
At round 192 accuracy: 0.6420664206642066
At round 192 training accuracy: 0.67
At round 192 training loss: 0.9479808188353975
gradient difference: 64.94000774808052
At round 193 accuracy: 0.6974169741697417
At round 193 training accuracy: 0.7194791666666667
At round 193 training loss: 0.8557302196354916
gradient difference: 59.74418723356352
At round 194 accuracy: 0.6992619926199262
At round 194 training accuracy: 0.7286458333333333
At round 194 training loss: 0.8591409657523036
gradient difference: 59.821459510060365
At round 195 accuracy: 0.6771217712177122
At round 195 training accuracy: 0.6909375
At round 195 training loss: 0.9414271187285582
gradient difference: 64.15093763966713
At round 196 accuracy: 0.5719557195571956
At round 196 training accuracy: 0.5938541666666667
At round 196 training loss: 1.3474338374333457
gradient difference: 82.52445181135658
At round 197 accuracy: 0.5479704797047971
At round 197 training accuracy: 0.5769791666666667
At round 197 training loss: 1.5034195069627216
gradient difference: 88.01177355981918
At round 198 accuracy: 0.5276752767527675
At round 198 training accuracy: 0.5588541666666667
At round 198 training loss: 1.900245173832712
gradient difference: 95.09017612374076
At round 199 accuracy: 0.7001845018450185
At round 199 training accuracy: 0.7297916666666666
At round 199 training loss: 0.836592711781462
gradient difference: 59.93989891375678
At round 200 accuracy: 0.6955719557195572
At round 200 training accuracy: 0.7308333333333333
