2022-08-19 09:57:52.831905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
WARNING:tensorflow:From /home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_1_1
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 1.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.
  warnings.warn('`tf.layers.dense` is deprecated and '
/home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.
  warnings.warn('`layer.apply` is deprecated and '
2022-08-19 09:57:55.041818: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-08-19 09:57:55.043331: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2022-08-19 09:57:55.436605: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2022-08-19 09:57:55.436652: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (node03): /proc/driver/nvidia/version does not exist
2022-08-19 09:57:55.436959: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-08-19 09:57:55.437047: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2022-08-19 09:57:55.438897: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)
2022-08-19 09:57:55.440307: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
WARNING:tensorflow:From /home/aig/.conda/envs/prox/lib/python3.9/site-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`
Incomplete shape.
Incomplete shape.

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================
Incomplete shape.
Incomplete shape.

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/4.84k flops)
  dense/kernel/Initializer/random_uniform (600/1.20k flops)
    dense/kernel/Initializer/random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/AssignSub (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  dense/kernel/Regularizer/Square (600/600 flops)
  dense/kernel/Regularizer/Sum (599/599 flops)
  PGD/update_dense/bias/AssignSub (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  dense/kernel/Regularizer/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
WARNING:tensorflow:From /home/aig/NailIt/FedProx/flearn/models/synthetic/mclr.py:58: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer Variable.assign which has equivalent behavior in 2.X.
At round 0 accuracy: 0.035977859778597784
At round 0 training accuracy: 0.035104166666666665
At round 0 training loss: 4.849575695159535
gradient difference: 146.9289105305644
At round 1 accuracy: 0.07749077490774908
At round 1 training accuracy: 0.080625
At round 1 training loss: 4.951505127685765
gradient difference: 152.05733737852205
At round 2 accuracy: 0.09317343173431734
At round 2 training accuracy: 0.09041666666666667
At round 2 training loss: 5.181022837646306
gradient difference: 151.36927626612538
At round 3 accuracy: 0.0996309963099631
At round 3 training accuracy: 0.09604166666666666
At round 3 training loss: 4.931657529932758
gradient difference: 151.75054643923173
At round 4 accuracy: 0.3985239852398524
At round 4 training accuracy: 0.4175
At round 4 training loss: 1.5782007198035717
gradient difference: 84.78866295251404
At round 5 accuracy: 0.3929889298892989
At round 5 training accuracy: 0.37333333333333335
At round 5 training loss: 1.5513577047859628
gradient difference: 90.41406539872163
At round 6 accuracy: 0.37177121771217714
At round 6 training accuracy: 0.373125
At round 6 training loss: 1.5734714932615559
gradient difference: 97.39910843454489
At round 7 accuracy: 0.42066420664206644
At round 7 training accuracy: 0.4214583333333333
At round 7 training loss: 1.531823177114129
gradient difference: 91.44149981989244
At round 8 accuracy: 0.3994464944649446
At round 8 training accuracy: 0.39479166666666665
At round 8 training loss: 1.656376199585696
gradient difference: 97.63868739985976
At round 9 accuracy: 0.5119926199261993
At round 9 training accuracy: 0.5321875
At round 9 training loss: 1.2884757361126442
gradient difference: 77.67967483664141
At round 10 accuracy: 0.45940959409594095
At round 10 training accuracy: 0.4841666666666667
At round 10 training loss: 1.2923591015425822
gradient difference: 79.99975291471611
At round 11 accuracy: 0.4566420664206642
At round 11 training accuracy: 0.4859375
At round 11 training loss: 1.349238311946392
gradient difference: 81.96909401168185
At round 12 accuracy: 0.4252767527675277
At round 12 training accuracy: 0.42291666666666666
At round 12 training loss: 1.4468343044817447
gradient difference: 100.93319379579981
At round 13 accuracy: 0.5175276752767528
At round 13 training accuracy: 0.5457291666666667
At round 13 training loss: 1.214088780687501
gradient difference: 77.3201907006339
At round 14 accuracy: 0.5350553505535055
At round 14 training accuracy: 0.5172916666666667
At round 14 training loss: 1.2378789633139968
gradient difference: 81.84053401007299
At round 15 accuracy: 0.5092250922509225
At round 15 training accuracy: 0.539375
At round 15 training loss: 1.2242443028651178
gradient difference: 79.57308228659502
At round 16 accuracy: 0.48154981549815495
At round 16 training accuracy: 0.5101041666666667
At round 16 training loss: 1.1922240291225414
gradient difference: 78.79964346740887
At round 17 accuracy: 0.4584870848708487
At round 17 training accuracy: 0.48625
At round 17 training loss: 1.2321935091229776
gradient difference: 82.5365610832687
At round 18 accuracy: 0.5138376383763837
At round 18 training accuracy: 0.5444791666666666
At round 18 training loss: 1.1219955212498705
gradient difference: 72.7300096250258
At round 19 accuracy: 0.49538745387453875
At round 19 training accuracy: 0.5226041666666666
At round 19 training loss: 1.1556804497167468
gradient difference: 73.67740068129255
At round 20 accuracy: 0.4907749077490775
At round 20 training accuracy: 0.5170833333333333
At round 20 training loss: 1.2129048568196594
gradient difference: 76.02649982403744
At round 21 accuracy: 0.49538745387453875
At round 21 training accuracy: 0.5278125
At round 21 training loss: 1.2626899784368772
gradient difference: 77.77935986507984
At round 22 accuracy: 0.4907749077490775
At round 22 training accuracy: 0.5204166666666666
At round 22 training loss: 1.314221946944793
gradient difference: 81.74716073022527
At round 23 accuracy: 0.6014760147601476
At round 23 training accuracy: 0.6088541666666667
At round 23 training loss: 1.0222918275619546
gradient difference: 68.4139025474369
At round 24 accuracy: 0.5987084870848709
At round 24 training accuracy: 0.61
At round 24 training loss: 1.0102477802274128
gradient difference: 68.38905715734003
At round 25 accuracy: 0.559040590405904
At round 25 training accuracy: 0.5417708333333333
At round 25 training loss: 1.0449665225545566
gradient difference: 73.28367290311483
At round 26 accuracy: 0.584870848708487
At round 26 training accuracy: 0.5695833333333333
At round 26 training loss: 1.009004377524058
gradient difference: 69.64039572628445
At round 27 accuracy: 0.5507380073800738
At round 27 training accuracy: 0.5317708333333333
At round 27 training loss: 1.0565032930051288
gradient difference: 72.8904490852592
At round 28 accuracy: 0.5682656826568265
At round 28 training accuracy: 0.5504166666666667
At round 28 training loss: 1.0045200918552777
gradient difference: 70.79098690241759
At round 29 accuracy: 0.5885608856088561
At round 29 training accuracy: 0.5836458333333333
At round 29 training loss: 0.9761834119322399
gradient difference: 67.71251416740465
At round 30 accuracy: 0.6190036900369004
At round 30 training accuracy: 0.6346875
At round 30 training loss: 0.9479580306603262
gradient difference: 63.731655599252996
At round 31 accuracy: 0.5867158671586716
At round 31 training accuracy: 0.5758333333333333
At round 31 training loss: 0.973506123488769
gradient difference: 68.81757448007859
At round 32 accuracy: 0.5987084870848709
At round 32 training accuracy: 0.6076041666666666
At round 32 training loss: 0.9504710641782731
gradient difference: 66.20825354938215
At round 33 accuracy: 0.5295202952029521
At round 33 training accuracy: 0.5653125
At round 33 training loss: 0.992968178652227
gradient difference: 70.48938973768749
At round 34 accuracy: 0.5092250922509225
At round 34 training accuracy: 0.5423958333333333
At round 34 training loss: 1.0353481435806802
gradient difference: 74.8530128831038
At round 35 accuracy: 0.5968634686346863
At round 35 training accuracy: 0.6311458333333333
At round 35 training loss: 0.9349293777346611
gradient difference: 64.68692705749814
At round 36 accuracy: 0.6070110701107011
At round 36 training accuracy: 0.6438541666666666
At round 36 training loss: 0.9065429453644902
gradient difference: 62.52871870086463
At round 37 accuracy: 0.6014760147601476
At round 37 training accuracy: 0.6429166666666667
At round 37 training loss: 0.9091348318196834
gradient difference: 63.01796085410989
At round 38 accuracy: 0.6254612546125461
At round 38 training accuracy: 0.6491666666666667
At round 38 training loss: 0.8945446765205513
gradient difference: 61.934646970599864
At round 39 accuracy: 0.6134686346863468
At round 39 training accuracy: 0.6547916666666667
At round 39 training loss: 0.8825056878632556
gradient difference: 60.8746891225592
At round 40 accuracy: 0.5571955719557196
At round 40 training accuracy: 0.5945833333333334
At round 40 training loss: 0.9396004907911023
gradient difference: 67.21607259286014
At round 41 accuracy: 0.5719557195571956
At round 41 training accuracy: 0.6133333333333333
At round 41 training loss: 0.9201765897093962
gradient difference: 65.179029180429
At round 42 accuracy: 0.6291512915129152
At round 42 training accuracy: 0.635625
At round 42 training loss: 0.9013539427084227
gradient difference: 63.50731875415832
At round 43 accuracy: 0.6171586715867159
At round 43 training accuracy: 0.6526041666666667
At round 43 training loss: 0.889542856986324
gradient difference: 62.201334083350645
At round 44 accuracy: 0.6309963099630996
At round 44 training accuracy: 0.6572916666666667
At round 44 training loss: 0.8744415421721836
gradient difference: 60.229267697170506
At round 45 accuracy: 0.6143911439114391
At round 45 training accuracy: 0.6505208333333333
At round 45 training loss: 0.8692912117143472
gradient difference: 59.41014340611773
At round 46 accuracy: 0.6226937269372693
At round 46 training accuracy: 0.6541666666666667
At round 46 training loss: 0.862620487610499
gradient difference: 59.2422233701356
At round 47 accuracy: 0.5922509225092251
At round 47 training accuracy: 0.63
At round 47 training loss: 0.8801868480443954
gradient difference: 61.20715473095347
At round 48 accuracy: 0.5673431734317343
At round 48 training accuracy: 0.6030208333333333
At round 48 training loss: 0.9139367494390657
gradient difference: 64.84488521394383
At round 49 accuracy: 0.6282287822878229
At round 49 training accuracy: 0.6183333333333333
At round 49 training loss: 0.8924746232914428
gradient difference: 63.42430458458965
At round 50 accuracy: 0.6208487084870848
At round 50 training accuracy: 0.6545833333333333
At round 50 training loss: 0.8567514284948508
gradient difference: 59.37373569924891
At round 51 accuracy: 0.6116236162361623
At round 51 training accuracy: 0.6517708333333333
At round 51 training loss: 0.8391804496385157
gradient difference: 57.86745885101387
At round 52 accuracy: 0.6143911439114391
At round 52 training accuracy: 0.650625
At round 52 training loss: 0.8413677175467213
gradient difference: 58.35644733729436
At round 53 accuracy: 0.6300738007380073
At round 53 training accuracy: 0.6710416666666666
At round 53 training loss: 0.8257232108339667
gradient difference: 56.80134530625161
At round 54 accuracy: 0.6429889298892989
At round 54 training accuracy: 0.639375
At round 54 training loss: 0.8572823435844232
gradient difference: 60.74355675846864
At round 55 accuracy: 0.6494464944649446
At round 55 training accuracy: 0.6839583333333333
At round 55 training loss: 0.805373672945425
gradient difference: 54.970265900829105
At round 56 accuracy: 0.6429889298892989
At round 56 training accuracy: 0.6852083333333333
At round 56 training loss: 0.793435147308434
gradient difference: 53.32311917048313
At round 57 accuracy: 0.6088560885608856
At round 57 training accuracy: 0.6555208333333333
At round 57 training loss: 0.8119987491052598
gradient difference: 54.98698106329998
At round 58 accuracy: 0.5793357933579336
At round 58 training accuracy: 0.6141666666666666
At round 58 training loss: 0.8584769359168907
gradient difference: 58.35748504421362
At round 59 accuracy: 0.6623616236162362
At round 59 training accuracy: 0.6661458333333333
At round 59 training loss: 0.7995136261017373
gradient difference: 53.70235713646278
At round 60 accuracy: 0.6356088560885609
At round 60 training accuracy: 0.6139583333333334
At round 60 training loss: 0.8469459773569057
gradient difference: 58.22856883238206
At round 61 accuracy: 0.6457564575645757
At round 61 training accuracy: 0.6198958333333333
At round 61 training loss: 0.8333821390879651
gradient difference: 56.63811937144878
At round 62 accuracy: 0.6346863468634686
At round 62 training accuracy: 0.6117708333333334
At round 62 training loss: 0.8357761754561216
gradient difference: 56.10734716695471
At round 63 accuracy: 0.5931734317343174
At round 63 training accuracy: 0.5840625
At round 63 training loss: 0.88763085679772
gradient difference: 61.25264571306642
At round 64 accuracy: 0.6217712177121771
At round 64 training accuracy: 0.6098958333333333
At round 64 training loss: 0.8382452846877277
gradient difference: 57.04967919365569
At round 65 accuracy: 0.665129151291513
At round 65 training accuracy: 0.6573958333333333
At round 65 training loss: 0.7813801387480149
gradient difference: 51.1802496299277
At round 66 accuracy: 0.6826568265682657
At round 66 training accuracy: 0.6871875
At round 66 training loss: 0.7441953257564455
gradient difference: 47.087483576416226
At round 67 accuracy: 0.6771217712177122
At round 67 training accuracy: 0.7064583333333333
At round 67 training loss: 0.7277867152759184
gradient difference: 45.38940306018831
At round 68 accuracy: 0.6798892988929889
At round 68 training accuracy: 0.6815625
At round 68 training loss: 0.7455764022469521
gradient difference: 47.43769074488443
At round 69 accuracy: 0.6789667896678967
At round 69 training accuracy: 0.6595833333333333
At round 69 training loss: 0.7616885356760273
gradient difference: 48.97044686737042
At round 70 accuracy: 0.6743542435424354
At round 70 training accuracy: 0.6535416666666667
At round 70 training loss: 0.7602277412513891
gradient difference: 49.51050976179997
At round 71 accuracy: 0.5571955719557196
At round 71 training accuracy: 0.5457291666666667
At round 71 training loss: 0.9821161580706637
gradient difference: 69.25794885715086
At round 72 accuracy: 0.6761992619926199
At round 72 training accuracy: 0.7021875
At round 72 training loss: 0.7189820602039496
gradient difference: 45.31211505430917
At round 73 accuracy: 0.6743542435424354
At round 73 training accuracy: 0.7039583333333334
At round 73 training loss: 0.7131738096506646
gradient difference: 45.02276786626667
At round 74 accuracy: 0.6743542435424354
At round 74 training accuracy: 0.6785416666666667
At round 74 training loss: 0.7271811000568171
gradient difference: 47.14084385689435
At round 75 accuracy: 0.6900369003690037
At round 75 training accuracy: 0.6842708333333334
At round 75 training loss: 0.7188789200441291
gradient difference: 45.63213337642085
At round 76 accuracy: 0.6706642066420664
At round 76 training accuracy: 0.7073958333333333
At round 76 training loss: 0.6982546840670208
gradient difference: 43.39880139091349
At round 77 accuracy: 0.6642066420664207
At round 77 training accuracy: 0.71
At round 77 training loss: 0.6975031073143085
gradient difference: 43.05856285799417
At round 78 accuracy: 0.5959409594095941
At round 78 training accuracy: 0.626875
At round 78 training loss: 0.7892022343724966
gradient difference: 52.47931419423942
At round 79 accuracy: 0.5913284132841329
At round 79 training accuracy: 0.5888541666666667
At round 79 training loss: 0.8607906276142846
gradient difference: 59.640837493256534
At round 80 accuracy: 0.5894833948339483
At round 80 training accuracy: 0.5882291666666667
At round 80 training loss: 0.8680245342043539
gradient difference: 60.230963682638276
At round 81 accuracy: 0.6605166051660517
At round 81 training accuracy: 0.641875
At round 81 training loss: 0.7663821313406031
gradient difference: 50.41574109376953
At round 82 accuracy: 0.6429889298892989
At round 82 training accuracy: 0.6847916666666667
At round 82 training loss: 0.7187502774813523
gradient difference: 45.06078854196091
At round 83 accuracy: 0.5765682656826568
At round 83 training accuracy: 0.6009375
At round 83 training loss: 0.8822491429032137
gradient difference: 59.17320440769455
At round 84 accuracy: 0.6780442804428044
At round 84 training accuracy: 0.663125
At round 84 training loss: 0.7427253831395259
gradient difference: 47.97248415308927
At round 85 accuracy: 0.6946494464944649
At round 85 training accuracy: 0.7048958333333334
At round 85 training loss: 0.7001194714196026
gradient difference: 43.6135736163542
At round 86 accuracy: 0.690959409594096
At round 86 training accuracy: 0.6845833333333333
At round 86 training loss: 0.7151786345119278
gradient difference: 43.85355679827599
At round 87 accuracy: 0.6964944649446494
At round 87 training accuracy: 0.7035416666666666
At round 87 training loss: 0.7000241013523191
gradient difference: 42.154596046642915
At round 88 accuracy: 0.6946494464944649
At round 88 training accuracy: 0.7259375
At round 88 training loss: 0.6731338699503492
gradient difference: 39.81127885839944
At round 89 accuracy: 0.6872693726937269
At round 89 training accuracy: 0.7317708333333334
At round 89 training loss: 0.6639536502057065
gradient difference: 38.65671486037513
At round 90 accuracy: 0.6872693726937269
At round 90 training accuracy: 0.7296875
At round 90 training loss: 0.6636546366661787
gradient difference: 38.87253212010668
At round 91 accuracy: 0.6863468634686347
At round 91 training accuracy: 0.7253125
At round 91 training loss: 0.6641542978088061
gradient difference: 38.47356545764994
At round 92 accuracy: 0.6559040590405905
At round 92 training accuracy: 0.65125
At round 92 training loss: 0.7475596635912856
gradient difference: 47.772148985776
At round 93 accuracy: 0.6466789667896679
At round 93 training accuracy: 0.6388541666666666
At round 93 training loss: 0.7663975048965465
gradient difference: 50.28947663827269
At round 94 accuracy: 0.7029520295202952
At round 94 training accuracy: 0.7053125
At round 94 training loss: 0.6763521956310918
gradient difference: 39.12591318125158
At round 95 accuracy: 0.6872693726937269
At round 95 training accuracy: 0.691875
At round 95 training loss: 0.6909989125576491
gradient difference: 40.332567441335485
At round 96 accuracy: 0.6872693726937269
At round 96 training accuracy: 0.6792708333333334
At round 96 training loss: 0.7015448910774043
gradient difference: 41.45247006245741
At round 97 accuracy: 0.6955719557195572
At round 97 training accuracy: 0.7104166666666667
At round 97 training loss: 0.6687337705555062
gradient difference: 39.31113184369759
At round 98 accuracy: 0.6476014760147601
At round 98 training accuracy: 0.6425
At round 98 training loss: 0.7470654861799751
gradient difference: 47.73458020551487
At round 99 accuracy: 0.6595940959409594
At round 99 training accuracy: 0.6533333333333333
At round 99 training loss: 0.7364871684663619
gradient difference: 46.64635596020269
At round 100 accuracy: 0.6900369003690037
At round 100 training accuracy: 0.72625
At round 100 training loss: 0.6528621095263709
gradient difference: 37.94660515938255
At round 101 accuracy: 0.6955719557195572
At round 101 training accuracy: 0.7140625
At round 101 training loss: 0.665150075815618
gradient difference: 39.91118176809586
At round 102 accuracy: 0.7011070110701108
At round 102 training accuracy: 0.7069791666666667
At round 102 training loss: 0.6684665545883278
gradient difference: 39.81632672048188
At round 103 accuracy: 0.7011070110701108
At round 103 training accuracy: 0.724375
At round 103 training loss: 0.6534420126195376
gradient difference: 38.724717546581545
At round 104 accuracy: 0.6014760147601476
At round 104 training accuracy: 0.5960416666666667
At round 104 training loss: 0.8602987585418547
gradient difference: 57.85317607797889
At round 105 accuracy: 0.7038745387453874
At round 105 training accuracy: 0.7090625
At round 105 training loss: 0.6667768475196014
gradient difference: 40.43197798543263
At round 106 accuracy: 0.6955719557195572
At round 106 training accuracy: 0.6990625
At round 106 training loss: 0.6732358129648491
gradient difference: 41.19113368004826
At round 107 accuracy: 0.6706642066420664
At round 107 training accuracy: 0.7092708333333333
At round 107 training loss: 0.6584954199784746
gradient difference: 39.7290240055022
At round 108 accuracy: 0.6512915129151291
At round 108 training accuracy: 0.7017708333333333
At round 108 training loss: 0.6696045744310444
gradient difference: 41.344064874704706
At round 109 accuracy: 0.6928044280442804
At round 109 training accuracy: 0.7357291666666667
At round 109 training loss: 0.6368962516946097
gradient difference: 37.90545446479021
At round 110 accuracy: 0.698339483394834
At round 110 training accuracy: 0.7366666666666667
At round 110 training loss: 0.6354814554937184
gradient difference: 37.807842222965085
At round 111 accuracy: 0.6955719557195572
At round 111 training accuracy: 0.7389583333333334
At round 111 training loss: 0.6333816200541332
gradient difference: 36.86880339642841
At round 112 accuracy: 0.698339483394834
At round 112 training accuracy: 0.7402083333333334
At round 112 training loss: 0.6319031442888081
gradient difference: 36.853325193887414
At round 113 accuracy: 0.6964944649446494
At round 113 training accuracy: 0.7383333333333333
At round 113 training loss: 0.6304363722012689
gradient difference: 36.85618240757743
At round 114 accuracy: 0.6854243542435424
At round 114 training accuracy: 0.7234375
At round 114 training loss: 0.6438436439074576
gradient difference: 37.81851567003767
At round 115 accuracy: 0.6992619926199262
At round 115 training accuracy: 0.7408333333333333
At round 115 training loss: 0.6290572801154728
gradient difference: 36.27655453227396
At round 116 accuracy: 0.6937269372693727
At round 116 training accuracy: 0.740625
At round 116 training loss: 0.6269131588796154
gradient difference: 35.87816260075711
At round 117 accuracy: 0.6642066420664207
At round 117 training accuracy: 0.7111458333333334
At round 117 training loss: 0.6520737062146267
gradient difference: 38.8188681459036
At round 118 accuracy: 0.6743542435424354
At round 118 training accuracy: 0.7155208333333334
At round 118 training loss: 0.6479487851448357
gradient difference: 38.26743375556686
At round 119 accuracy: 0.6918819188191881
At round 119 training accuracy: 0.7340625
At round 119 training loss: 0.6359188449072342
gradient difference: 37.121560165334564
At round 120 accuracy: 0.683579335793358
At round 120 training accuracy: 0.7266666666666667
At round 120 training loss: 0.6405253635315845
gradient difference: 38.00448694528237
At round 121 accuracy: 0.6872693726937269
At round 121 training accuracy: 0.7285416666666666
At round 121 training loss: 0.6382313806392873
gradient difference: 37.87813317537107
At round 122 accuracy: 0.6918819188191881
At round 122 training accuracy: 0.7401041666666667
At round 122 training loss: 0.6284067453164607
gradient difference: 37.28992677728415
At round 123 accuracy: 0.6808118081180812
At round 123 training accuracy: 0.7217708333333334
At round 123 training loss: 0.6432769903183604
gradient difference: 38.87782510594252
At round 124 accuracy: 0.6826568265682657
At round 124 training accuracy: 0.724375
At round 124 training loss: 0.6402991880119467
gradient difference: 38.48472501852166
At round 125 accuracy: 0.6734317343173432
At round 125 training accuracy: 0.7175
At round 125 training loss: 0.6466547508786122
gradient difference: 39.06991504343432
At round 126 accuracy: 0.6688191881918819
At round 126 training accuracy: 0.7125
At round 126 training loss: 0.6501627831595639
gradient difference: 39.34595021705129
At round 127 accuracy: 0.683579335793358
At round 127 training accuracy: 0.7273958333333334
At round 127 training loss: 0.6406187089455003
gradient difference: 37.882847726626636
At round 128 accuracy: 0.6761992619926199
At round 128 training accuracy: 0.7179166666666666
At round 128 training loss: 0.6462924073652054
gradient difference: 38.234095429241926
At round 129 accuracy: 0.6512915129151291
At round 129 training accuracy: 0.6967708333333333
At round 129 training loss: 0.6789786969753914
gradient difference: 40.99674716320973
At round 130 accuracy: 0.6383763837638377
At round 130 training accuracy: 0.6707291666666667
At round 130 training loss: 0.7055114615700828
gradient difference: 43.882032408903335
At round 131 accuracy: 0.6383763837638377
At round 131 training accuracy: 0.6755208333333333
At round 131 training loss: 0.6956049749914868
gradient difference: 43.14877158440113
At round 132 accuracy: 0.6900369003690037
At round 132 training accuracy: 0.7360416666666667
At round 132 training loss: 0.6282501438252317
gradient difference: 36.365698106427516
At round 133 accuracy: 0.6964944649446494
At round 133 training accuracy: 0.7439583333333334
At round 133 training loss: 0.6227736194594763
gradient difference: 35.240793905916554
At round 134 accuracy: 0.6964944649446494
At round 134 training accuracy: 0.7414583333333333
At round 134 training loss: 0.6249352225707844
gradient difference: 35.486056252720275
At round 135 accuracy: 0.6946494464944649
At round 135 training accuracy: 0.743125
At round 135 training loss: 0.6233729476737789
gradient difference: 35.676494799608655
At round 136 accuracy: 0.6143911439114391
At round 136 training accuracy: 0.6494791666666667
At round 136 training loss: 0.7360986918754254
gradient difference: 46.83495373991634
At round 137 accuracy: 0.6134686346863468
At round 137 training accuracy: 0.6452083333333334
At round 137 training loss: 0.750850759082629
gradient difference: 47.583471823844256
At round 138 accuracy: 0.6088560885608856
At round 138 training accuracy: 0.64625
At round 138 training loss: 0.7459806755379152
gradient difference: 47.26891410417432
At round 139 accuracy: 0.6974169741697417
At round 139 training accuracy: 0.7410416666666667
At round 139 training loss: 0.6185545332279677
gradient difference: 35.21893801583683
At round 140 accuracy: 0.698339483394834
At round 140 training accuracy: 0.7416666666666667
At round 140 training loss: 0.6135470109293237
gradient difference: 34.28457396283914
At round 141 accuracy: 0.6918819188191881
At round 141 training accuracy: 0.7322916666666667
At round 141 training loss: 0.6162445675209165
gradient difference: 34.391699174628464
At round 142 accuracy: 0.7011070110701108
At round 142 training accuracy: 0.7394791666666667
At round 142 training loss: 0.6107318010072534
gradient difference: 33.662669913526095
At round 143 accuracy: 0.7011070110701108
At round 143 training accuracy: 0.7379166666666667
At round 143 training loss: 0.6083619989966974
gradient difference: 33.226475192064115
At round 144 accuracy: 0.6992619926199262
At round 144 training accuracy: 0.740625
At round 144 training loss: 0.6088245627718667
gradient difference: 33.418632125471596
At round 145 accuracy: 0.6531365313653137
At round 145 training accuracy: 0.6873958333333333
At round 145 training loss: 0.6680389818440502
gradient difference: 39.665929056574896
At round 146 accuracy: 0.6346863468634686
At round 146 training accuracy: 0.6664583333333334
At round 146 training loss: 0.7014712304272689
gradient difference: 42.63433003437436
At round 147 accuracy: 0.6309963099630996
At round 147 training accuracy: 0.6685416666666667
At round 147 training loss: 0.6969115912013998
gradient difference: 41.92744476043902
At round 148 accuracy: 0.6134686346863468
At round 148 training accuracy: 0.6528125
At round 148 training loss: 0.7263410910451785
gradient difference: 44.575201995211195
At round 149 accuracy: 0.6660516605166051
At round 149 training accuracy: 0.7104166666666667
At round 149 training loss: 0.6357115976714218
gradient difference: 36.148126083014674
At round 150 accuracy: 0.6660516605166051
At round 150 training accuracy: 0.7151041666666667
At round 150 training loss: 0.6281123896866726
gradient difference: 34.77824323244061
At round 151 accuracy: 0.6005535055350554
At round 151 training accuracy: 0.5986458333333333
At round 151 training loss: 0.8866171504425195
gradient difference: 56.159511967252556
At round 152 accuracy: 0.7084870848708487
At round 152 training accuracy: 0.749375
At round 152 training loss: 0.5928075236097599
gradient difference: 31.127874158571895
At round 153 accuracy: 0.7020295202952029
At round 153 training accuracy: 0.7403125
At round 153 training loss: 0.6021855924014623
gradient difference: 32.316711056257105
At round 154 accuracy: 0.7038745387453874
At round 154 training accuracy: 0.7471875
At round 154 training loss: 0.5962804054577524
gradient difference: 31.863484075024964
At round 155 accuracy: 0.7011070110701108
At round 155 training accuracy: 0.739375
At round 155 training loss: 0.6017050053187025
gradient difference: 32.56697247033247
At round 156 accuracy: 0.6992619926199262
At round 156 training accuracy: 0.7452083333333334
At round 156 training loss: 0.5974902210997728
gradient difference: 32.21789957977045
At round 157 accuracy: 0.6808118081180812
At round 157 training accuracy: 0.7228125
At round 157 training loss: 0.6134072945274723
gradient difference: 33.72697103837284
At round 158 accuracy: 0.7084870848708487
At round 158 training accuracy: 0.7
At round 158 training loss: 0.644509302637695
gradient difference: 36.87052886217258
At round 159 accuracy: 0.7011070110701108
At round 159 training accuracy: 0.7490625
At round 159 training loss: 0.5911539752843479
gradient difference: 31.512781925334494
At round 160 accuracy: 0.6937269372693727
At round 160 training accuracy: 0.7438541666666667
At round 160 training loss: 0.5937592926171298
gradient difference: 32.09247781413358
At round 161 accuracy: 0.7029520295202952
At round 161 training accuracy: 0.7489583333333333
At round 161 training loss: 0.5857170087805328
gradient difference: 30.521706061592056
At round 162 accuracy: 0.705719557195572
At round 162 training accuracy: 0.7526041666666666
At round 162 training loss: 0.5836067784616413
gradient difference: 29.326941435854994
At round 163 accuracy: 0.7066420664206642
At round 163 training accuracy: 0.750625
At round 163 training loss: 0.585249313674091
gradient difference: 29.68032557405744
At round 164 accuracy: 0.7084870848708487
At round 164 training accuracy: 0.7527083333333333
At round 164 training loss: 0.5813215286075137
gradient difference: 28.613303897588754
At round 165 accuracy: 0.7075645756457565
At round 165 training accuracy: 0.694375
At round 165 training loss: 0.6502698511638058
gradient difference: 35.668366894257375
At round 166 accuracy: 0.7121771217712177
At round 166 training accuracy: 0.7158333333333333
At round 166 training loss: 0.6189937641758783
gradient difference: 32.942475450989384
At round 167 accuracy: 0.7149446494464945
At round 167 training accuracy: 0.7335416666666666
At round 167 training loss: 0.600276835062541
gradient difference: 30.852011328067256
At round 168 accuracy: 0.716789667896679
At round 168 training accuracy: 0.735625
At round 168 training loss: 0.5977455218927935
gradient difference: 30.5322201244085
At round 169 accuracy: 0.7130996309963099
At round 169 training accuracy: 0.7441666666666666
At round 169 training loss: 0.5884800513461232
gradient difference: 29.235421064055377
At round 170 accuracy: 0.7158671586715867
At round 170 training accuracy: 0.7342708333333333
At round 170 training loss: 0.5983962439776709
gradient difference: 30.14342161501736
At round 171 accuracy: 0.7112546125461254
At round 171 training accuracy: 0.7532291666666666
At round 171 training loss: 0.5779336705982374
gradient difference: 27.817619486394204
At round 172 accuracy: 0.709409594095941
At round 172 training accuracy: 0.755
At round 172 training loss: 0.5791076086779746
gradient difference: 28.292782308080653
At round 173 accuracy: 0.7130996309963099
At round 173 training accuracy: 0.7569791666666666
At round 173 training loss: 0.581272615410077
gradient difference: 28.704749094620507
At round 174 accuracy: 0.7149446494464945
At round 174 training accuracy: 0.7536458333333333
At round 174 training loss: 0.5839021438453347
gradient difference: 28.96191986074512
At round 175 accuracy: 0.7130996309963099
At round 175 training accuracy: 0.7560416666666666
At round 175 training loss: 0.5794595773983747
gradient difference: 28.245636935265374
At round 176 accuracy: 0.7047970479704797
At round 176 training accuracy: 0.7566666666666667
At round 176 training loss: 0.5788085284254824
gradient difference: 28.291522017045057
At round 177 accuracy: 0.6854243542435424
At round 177 training accuracy: 0.7336458333333333
At round 177 training loss: 0.5949510593072046
gradient difference: 29.73058368987952
At round 178 accuracy: 0.6826568265682657
At round 178 training accuracy: 0.7341666666666666
At round 178 training loss: 0.5934300697866517
gradient difference: 29.516649299266867
At round 179 accuracy: 0.6586715867158671
At round 179 training accuracy: 0.701875
At round 179 training loss: 0.6363057278577859
gradient difference: 33.94161875783684
At round 180 accuracy: 0.6356088560885609
At round 180 training accuracy: 0.6732291666666667
At round 180 training loss: 0.6781450400874018
gradient difference: 37.42042696809735
At round 181 accuracy: 0.665129151291513
At round 181 training accuracy: 0.7045833333333333
At round 181 training loss: 0.633070108689523
gradient difference: 33.14146287731193
At round 182 accuracy: 0.6586715867158671
At round 182 training accuracy: 0.69375
At round 182 training loss: 0.647295016827217
gradient difference: 34.74880369785971
At round 183 accuracy: 0.7214022140221402
At round 183 training accuracy: 0.7321875
At round 183 training loss: 0.5988896444470933
gradient difference: 29.963027464257504
At round 184 accuracy: 0.7260147601476015
At round 184 training accuracy: 0.7302083333333333
At round 184 training loss: 0.6033497393915119
gradient difference: 30.580628665977528
At round 185 accuracy: 0.7214022140221402
At round 185 training accuracy: 0.735625
At round 185 training loss: 0.5931102429439004
gradient difference: 28.975683709491175
At round 186 accuracy: 0.724169741697417
At round 186 training accuracy: 0.7438541666666667
At round 186 training loss: 0.5871359351913755
gradient difference: 28.411930373519848
At round 187 accuracy: 0.6761992619926199
At round 187 training accuracy: 0.7235416666666666
At round 187 training loss: 0.6032166457106359
gradient difference: 30.271624027395767
At round 188 accuracy: 0.6817343173431735
At round 188 training accuracy: 0.7228125
At round 188 training loss: 0.6033350322659438
gradient difference: 30.069302628974924
At round 189 accuracy: 0.6928044280442804
At round 189 training accuracy: 0.7384375
At round 189 training loss: 0.5857351534285893
gradient difference: 28.337137356927165
At round 190 accuracy: 0.6863468634686347
At round 190 training accuracy: 0.7352083333333334
At round 190 training loss: 0.5889948131764928
gradient difference: 28.289872207112285
At round 191 accuracy: 0.6171586715867159
At round 191 training accuracy: 0.6472916666666667
At round 191 training loss: 0.7527896277427983
gradient difference: 42.63950441867041
At round 192 accuracy: 0.6254612546125461
At round 192 training accuracy: 0.6554166666666666
At round 192 training loss: 0.7186832606916627
gradient difference: 40.208073643110204
At round 193 accuracy: 0.7223247232472325
At round 193 training accuracy: 0.7635416666666667
At round 193 training loss: 0.5625573475953812
gradient difference: 25.670008715833724
At round 194 accuracy: 0.698339483394834
At round 194 training accuracy: 0.7392708333333333
At round 194 training loss: 0.5813065762453091
gradient difference: 27.96748733728156
At round 195 accuracy: 0.709409594095941
At round 195 training accuracy: 0.6964583333333333
At round 195 training loss: 0.6455343572158987
gradient difference: 34.13645935540672
At round 196 accuracy: 0.7140221402214022
At round 196 training accuracy: 0.7108333333333333
At round 196 training loss: 0.6197950887974972
gradient difference: 31.40144882241353
At round 197 accuracy: 0.7177121771217713
At round 197 training accuracy: 0.7211458333333334
At round 197 training loss: 0.6066292132491556
gradient difference: 30.889839556012262
At round 198 accuracy: 0.7232472324723247
At round 198 training accuracy: 0.7422916666666667
At round 198 training loss: 0.5827802926806422
gradient difference: 28.20904420917501
At round 199 accuracy: 0.6642066420664207
At round 199 training accuracy: 0.6565625
At round 199 training loss: 0.7161991707095876
gradient difference: 40.45495585352173
At round 200 accuracy: 0.6642066420664207
At round 200 training accuracy: 0.7079166666666666
